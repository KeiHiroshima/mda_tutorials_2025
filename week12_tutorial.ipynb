{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 第12回演習：Retrieval Augmented Generation (RAG) の構築\n",
    "#\n",
    "# 本演習では，これまでの講義で学んだ大規模言語モデル (LLM) の知識を活かし，\n",
    "# 講義資料 (`week12_2025.pdf`) の内容に基づいて質問に回答する **RAG (Retrieval Augmented Generation)** システムを構築します．\n",
    "#\n",
    "# **本演習の目的:**\n",
    "# 1. 未学習のデータ（講義資料）を LLM に扱わせる手法としての RAG を理解する．\n",
    "# 2. PDF からのテキスト抽出，ベクトル化，検索，そして生成という RAG の一連のパイプラインを実装する．\n",
    "# 3. Google Colab の無料枠 (T4 GPU) で動作する軽量かつ高性能なモデル (`Qwen2.5-1.5B`) を体験する．\n",
    "#\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc4372",
   "metadata": {},
   "source": [
    "## 0. 準備: ライブラリのインストールと環境設定\n",
    "\n",
    "RAG の構築に必要なライブラリをインストールします．\n",
    "\n",
    "- `pypdf`: PDF ファイルからテキストを抽出するために使用します．\n",
    "- `sentence-transformers`: テキストをベクトル化（埋め込み表現に変換）するために使用します．\n",
    "- `transformers`, `accelerate`, `bitsandbytes`: LLM をロードし，高速に推論するために使用します．\n",
    "- `langchain` (今回は簡易実装のため使いませんが、発展的な実装には便利です)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a86c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install --system -q pypdf sentence-transformers transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768182f",
   "metadata": {},
   "source": [
    "必要なライブラリをインポートし，デバイス（GPU）の設定を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c504ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "# デバイスの設定 (GPUが使えるならcuda, 使えなければcpu)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973ebd5",
   "metadata": {},
   "source": [
    "## 1. データ読み込み: 講義資料 (PDF) の準備\n",
    "\n",
    "Google Colab で実行する場合，講義資料 (`week12_2025.pdf`) をアップロードする必要があります．\n",
    "以下のセルを実行して，ファイルをアップロードしてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# ファイルが見つからない場合はアップロードを促す\n",
    "pdf_filename = \"week12_2025.pdf\"\n",
    "if not os.path.exists(pdf_filename):\n",
    "    print(f\"'{pdf_filename}' が見つかりません．アップロードしてください．\")\n",
    "    uploaded = files.upload()\n",
    "    # アップロードされたファイル名を所得（複数可だが今回は1つと仮定）\n",
    "    for k in uploaded.keys():\n",
    "        pdf_filename = k\n",
    "        print(f\"アップロード完了: {pdf_filename}\")\n",
    "else:\n",
    "    print(f\"'{pdf_filename}' は既に存在します．\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdcc75",
   "metadata": {},
   "source": [
    "## 2. テキスト抽出\n",
    "\n",
    "アップロードされたPDFを読み込み，テキストデータを抽出します．\n",
    "機械にとって扱いやすいように，意味のまとまり（チャンク）ごとに分割する処理も重要ですが，\n",
    "今回はシンプルにページごと，あるいは一定の文字数で分割して扱います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef09665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFファイルのパス (Colab等の環境に合わせて適宜変更してください)\n",
    "pdf_path = pdf_filename\n",
    "\n",
    "def load_pdf_text(path):\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "pdf_content = load_pdf_text(pdf_path)\n",
    "print(f\"抽出された文字数: {len(pdf_content)} 文字\")\n",
    "print(\"--- 先頭500文字プレビュー ---\")\n",
    "print(pdf_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928a573",
   "metadata": {},
   "source": [
    "### テキストのチャンキング (分割)\n",
    "\n",
    "長いテキストをそのまま扱うと，埋め込みモデルの入力制限を超えたり，検索精度が落ちたりします．\n",
    "ここでは，簡易的に **300文字** 程度のチャンクに分割し，少しずつオーバーラップさせます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(pdf_content)\n",
    "print(f\"チャンク数: {len(chunks)}\")\n",
    "print(f\"チャンク例: {chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19243188",
   "metadata": {},
   "source": [
    "## 3. 検索 (Retrieval) の準備: 埋め込みモデルのロードとデータベース構築\n",
    "\n",
    "テキストの意味をベクトル（数値の列）に変換する「埋め込みモデル (Embedding Model)」を使用します．\n",
    "今回は日本語にも対応しており，高性能な `intfloat/multilingual-e5-large` を使用します．\n",
    "\n",
    "**注意:** `e5` モデルは，検索対象の文章には `passage: `，クエリには `query: ` という接頭辞（プレフィックス）をつけることが推奨されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e227c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みモデルのロード\n",
    "embed_model_name = \"intfloat/multilingual-e5-large\"\n",
    "print(f\"Loading embedding model: {embed_model_name}...\")\n",
    "embedder = SentenceTransformer(embed_model_name, device=device)\n",
    "\n",
    "# ドキュメント（チャンク）のベクトル化\n",
    "# prefix \"passage: \" を付与して埋め込む\n",
    "passage_chunks = [\"passage: \" + c for c in chunks]\n",
    "corpus_embeddings = embedder.encode(passage_chunks, convert_to_tensor=True)\n",
    "\n",
    "print(\"Vector database creation complete.\")\n",
    "print(f\"Embedding shape: {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c4627",
   "metadata": {},
   "source": [
    "## 4. 生成 (Generation) の準備: LLM のロード\n",
    "\n",
    "検索した情報を元に回答を生成する LLM をロードします．\n",
    "Google Colab の無料枠でも動作し，かつ日本語性能が高い **Qwen2.5-1.5B-Instruct** を使用します．\n",
    "これは 15億パラメータという小規模なモデルですが，非常に高い性能を持っています．\n",
    "\n",
    "※ Hugging Face のアカウント認証は不要なモデルです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18554f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMのロード\n",
    "llm_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "print(f\"Loading LLM: {llm_model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1007fa4",
   "metadata": {},
   "source": [
    "## 5. RAG システムの実装\n",
    "\n",
    "これまでの要素を組み合わせて，RAG の関数を作成します．\n",
    "\n",
    "**処理の流れ:**\n",
    "1. ユーザーの質問 (Query) を受け取る．\n",
    "2. 質問をベクトル化する (`query: ` を付与)．\n",
    "3. 事前に作成したデータベースから，質問に類似したチャンク（講義資料の一部）を検索する．\n",
    "4. 検索されたチャンクを「参考情報」としてプロンプトに組み込む．\n",
    "5. LLM にプロンプトを渡し，回答を生成させる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc222f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, k=3):\n",
    "    \"\"\"\n",
    "    クエリに関連するチャンクを上位k個検索する関数\n",
    "    \"\"\"\n",
    "    query_embedding = embedder.encode(f\"query: {query}\", convert_to_tensor=True)\n",
    "\n",
    "    # コサイン類似度を計算\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\n",
    "    # 上位k個のインデックスを取得\n",
    "    top_results = torch.topk(cos_scores, k=k)\n",
    "\n",
    "    relevant_chunks = []\n",
    "    for score, idx in zip(top_results.values, top_results.indices):\n",
    "        # チャンクのリストから元のテキストを取得 (passage: を除去して表示)\n",
    "        original_text = chunks[idx]\n",
    "        relevant_chunks.append(original_text)\n",
    "\n",
    "    return relevant_chunks\n",
    "\n",
    "def generate_rag_answer(query):\n",
    "    \"\"\"\n",
    "    RAG を用いて回答を生成する関数\n",
    "    \"\"\"\n",
    "    # 1. 検索\n",
    "    relevant_contexts = retrieve_relevant_chunks(query)\n",
    "    context_str = \"\\n\\n\".join(relevant_contexts)\n",
    "\n",
    "    # 2. プロンプト作成\n",
    "    # 文脈を与えて回答させるためのテンプレート\n",
    "    prompt_template = f\"\"\"以下は講義資料からの抜粋です。この内容に基づいて、ユーザーの質問に日本語で答えてください。\n",
    "資料に書かれていないことは「資料には記載がありません」と答えてください。\n",
    "\n",
    "[参考資料]\n",
    "{context_str}\n",
    "\n",
    "[質問]\n",
    "{query}\n",
    "\n",
    "[回答]\n",
    "\"\"\"\n",
    "\n",
    "    # チャット形式のフォーマットに変換\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"あなたは親切なAIアシスタントです。与えられた参考資料に基づいて、正確に質問に答えてください。\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 3. 生成\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response, relevant_contexts\n",
    "\n",
    "def generate_no_rag_answer(query):\n",
    "    \"\"\"\n",
    "    RAG を使用せずに（文脈なしで）回答を生成する関数\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"あなたは親切なAIアシスタントです。ユーザーの質問に日本語で答えてください。\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e0ae3",
   "metadata": {},
   "source": [
    "## 6. 実行と確認\n",
    "\n",
    "実際に講義内容について質問してみましょう．\n",
    "ここでは，PDFの内容（と想定されるもの）について質問を投げかけます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76325372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問の例\n",
    "questions = [\n",
    "    \"今回の講義の主要なテーマは何ですか？\",\n",
    "    \"大規模言語モデルの課題について、資料ではどのように述べられていますか？\",\n",
    "    \"次回の演習内容は決まっていますか？\",\n",
    "    \"LLMのマルチモーダル化とは具体的にどのようなことですか？\",\n",
    "    \"AIエージェントが研究もできるとはどういう意味ですか？\",\n",
    "    \"高度なLLMの活用方法として、どのような事例が紹介されていますか？\"\n",
    "]\n",
    "\n",
    "print(\"=== RAG vs No-RAG 比較デモ開始 ===\")\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # RAGなし\n",
    "    print(\"[No-RAG] 回答生成中...\")\n",
    "    no_rag_answer = generate_no_rag_answer(q)\n",
    "    print(\"A (No-RAG):\")\n",
    "    print(textwrap.fill(no_rag_answer, width=80))\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # RAGあり\n",
    "    print(\"[RAG] 回答生成中...\")\n",
    "    rag_answer, contexts = generate_rag_answer(q)\n",
    "    print(\"A (RAG):\")\n",
    "    print(textwrap.fill(rag_answer, width=80))\n",
    "\n",
    "    print(\"\\n[参照した情報の抜粋]\")\n",
    "    for i, ctx in enumerate(contexts):\n",
    "        print(f\"({i+1}) {ctx[:50]}...\") # 長いので先頭だけ表示"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
