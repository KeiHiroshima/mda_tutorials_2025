"""
AIãƒ¢ãƒ‡ãƒ«ã‚’æ”¯ãˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ - æ¼”ç¿’è³‡æ–™
MDAå…¥é–€ ç¬¬12å›

æœ¬æ¼”ç¿’ã§ã¯ä»¥ä¸‹ã®å†…å®¹ã‚’æ‰±ã„ã¾ã™:
1. ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
2. CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
4. CPU/GPU å­¦ç¿’æ™‚é–“ã®æ¯”è¼ƒ (MNIST)

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
- numpy
- cupy (GPUãŒã‚ã‚‹å ´åˆ)
- matplotlib
- torch
- psutil
- torchvision
"""

import subprocess
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import psutil
import torch
import torch.nn as nn

try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

warnings.filterwarnings("ignore")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "DejaVu Sans"]

print("\nãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
print(f"PyTorch version: {torch.__version__}")
print(f"NumPy version: {np.__version__}")

# =====================================
# æ¼”ç¿’1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹è¨ˆç®—ç’°å¢ƒã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚
- GPUæƒ…å ±
- CPUæƒ…å ±
- ãƒ¡ãƒ¢ãƒªæƒ…å ±
- PyTorchã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®š

â€» Windowsã®å ´åˆã€ã‚¹ãƒšãƒƒã‚¯æƒ…å ±ã‚’ç¢ºèªã™ã‚‹ã«ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
  > systeminfo
  ã¾ãŸã¯
  > dxdiag
"""
)

# --- GPUæƒ…å ±ã®ç¢ºèª ---
print("\nã€GPUæƒ…å ±ã€‘")
print("-" * 70)

if torch.cuda.is_available():
    print("âœ“ CUDA is available")
    print(f"âœ“ CUDA version: {torch.version.cuda}")
    print(f"âœ“ Number of GPUs: {torch.cuda.device_count()}")

    for i in range(torch.cuda.device_count()):
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        gpu_memory = torch.cuda.get_device_properties(i).total_memory
        print(f"  Total Memory: {gpu_memory / 1e9:.2f} GB")

    # nvidia-smiã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
    print("\nã€nvidia-smi å‡ºåŠ›ã€‘")
    try:
        result = subprocess.run(
            ["nvidia-smi"], capture_output=True, text=True, check=True
        )
        print(result.stdout)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("nvidia-smi ã‚³ãƒãƒ³ãƒ‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

    device = torch.device("cuda")
    print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device} (GPU)")
else:
    print("âœ— CUDA is not available")
    print("CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
    device = torch.device("cpu")
    print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device} (CPU)")

# --- CPUæƒ…å ±ã®ç¢ºèª ---
print("\nã€CPUæƒ…å ±ã€‘")
print("-" * 70)

# lscpuã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆLinux/Macï¼‰
print("ã€CPUè©³ç´°æƒ…å ±ã€‘")
try:
    result = subprocess.run(
        ["lscpu"], capture_output=True, text=True, check=True, timeout=5
    )
    # é‡è¦ãªæƒ…å ±ã®ã¿æŠ½å‡º
    for line in result.stdout.split("\n"):
        if any(
            keyword in line
            for keyword in [
                "Model name",
                "CPU(s)",
                "Thread(s)",
                "Core(s)",
                "Socket(s)",
                "CPU MHz",
            ]
        ):
            print(line)
except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
    # Windowsã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰å¤±æ•—æ™‚ã¯pythonã§å–å¾—
    print(f"CPU ã‚³ã‚¢æ•°: {psutil.cpu_count(logical=False)} (ç‰©ç†)")
    print(f"CPU ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {psutil.cpu_count(logical=True)} (è«–ç†)")
    print(f"CPU ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1)}%")

# --- ãƒ¡ãƒ¢ãƒªæƒ…å ±ã®ç¢ºèª ---
print("\nã€ãƒ¡ãƒ¢ãƒªæƒ…å ±ã€‘")
print("-" * 70)

ram = psutil.virtual_memory()
print(f"Total RAM: {ram.total / 1e9:.2f} GB")
print(f"Available RAM: {ram.available / 1e9:.2f} GB")
print(f"Used RAM: {ram.used / 1e9:.2f} GB")
print(f"RAM Usage: {ram.percent}%")

# GPU Memory
if torch.cuda.is_available():
    print("\nGPU Memory (Device 0):")
    print(f"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")


# =====================================
# æ¼”ç¿’2: CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’2: CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“")
print("=" * 70)

if not GPU_AVAILABLE:
    print("\nâš ï¸ CuPyãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã“ã®æ¼”ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
    print("Google Colabãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
else:
    print(
        """
ã“ã®æ¼”ç¿’ã§ã¯ã€CPUã¨GPUã§åŒã˜è¡Œåˆ—ç©ã®è¨ˆç®—ã‚’è¡Œã„ã€é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
- NumPy (CPU) ã¨ CuPy (GPU) ã‚’ä½¿ç”¨
- è¡Œåˆ—ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦å®Ÿé¨“
- è¨ˆç®—æ™‚é–“ã‚’æ¸¬å®š
"""
    )

    # è¡Œåˆ—ã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ
    sizes = [100, 500, 1000, 2000, 5000]
    cpu_times = []
    gpu_times = []
    # speedups = [] # é«˜é€ŸåŒ–ç‡ã®ã‚°ãƒ©ãƒ•ã¯å‰Šé™¤ã™ã‚‹ãŸã‚ãƒªã‚¹ãƒˆã‚‚ä¸è¦ã ãŒã€è¨ˆç®—çµæœã®è¡¨ç¤ºã«ã¯ä½¿ã†

    print("\nè¡Œåˆ—ç©ã®è¨ˆç®—é€Ÿåº¦ã‚’æ¸¬å®šä¸­...")
    print("-" * 70)

    for n in sizes:
        print(f"\nè¡Œåˆ—ã‚µã‚¤ã‚º: {n} Ã— {n}")

        # --- CPU (NumPy) ---
        A_cpu = np.random.rand(n, n).astype(np.float32)
        B_cpu = np.random.rand(n, n).astype(np.float32)

        start = time.time()
        C_cpu = A_cpu @ B_cpu
        cpu_time = time.time() - start
        cpu_times.append(cpu_time)
        print(f"  CPU (NumPy):  {cpu_time:.4f} ç§’")

        # --- GPU (CuPy) ---
        A_gpu = cp.random.rand(n, n).astype(cp.float32)
        B_gpu = cp.random.rand(n, n).astype(cp.float32)

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›ã¯é…ã„ã“ã¨ãŒã‚ã‚‹ãŸã‚ï¼‰
        _ = A_gpu @ B_gpu
        cp.cuda.Stream.null.synchronize()

        start = time.time()
        C_gpu = A_gpu @ B_gpu
        cp.cuda.Stream.null.synchronize()  # GPUè¨ˆç®—ã®å®Œäº†ã‚’å¾…ã¤
        gpu_time = time.time() - start
        gpu_times.append(gpu_time)
        print(f"  GPU (CuPy):   {gpu_time:.4f} ç§’")

        # é«˜é€ŸåŒ–ç‡
        speedup = cpu_time / gpu_time
        # speedups.append(speedup)
        print(f"  é«˜é€ŸåŒ–ç‡:     {speedup:.1f}x")

        # çµæœã®æ¤œè¨¼ï¼ˆè¨ˆç®—çµæœãŒæ­£ã—ã„ã‹ç¢ºèªï¼‰
        diff = np.abs(C_cpu - cp.asnumpy(C_gpu)).max()
        print(f"  æœ€å¤§èª¤å·®:     {diff:.2e}")

    # --- çµæœã®å¯è¦–åŒ– ---
    # å·¦å´ã®ã‚°ãƒ©ãƒ•ã®ã¿è¡¨ç¤ºã™ã‚‹ãŸã‚ã€subplotã§ã¯ãªãå˜ä¸€ã®plotã«ã™ã‚‹
    fig, ax = plt.subplots(figsize=(8, 6))

    # è¨ˆç®—æ™‚é–“ã®æ¯”è¼ƒ
    x_pos = np.arange(len(sizes))
    width = 0.35

    ax.bar(x_pos - width / 2, cpu_times, width, label="CPU (NumPy)", color="steelblue")
    ax.bar(x_pos + width / 2, gpu_times, width, label="GPU (CuPy)", color="coral")

    ax.set_xlabel("Matrix Size", fontsize=12)
    ax.set_ylabel("Time (seconds)", fontsize=12)
    ax.set_title(
        "CPU vs GPU: Matrix Multiplication Time", fontsize=14, fontweight="bold"
    )
    ax.set_xticks(x_pos)
    ax.set_xticklabels([f"{s}Ã—{s}" for s in sizes])
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis="y")
    ax.set_yscale("log")  # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«

    plt.tight_layout()
    plt.savefig("exercise2_cpu_gpu_comparison.png", dpi=150, bbox_inches="tight")
    print("\nå›³ã‚’ 'exercise2_cpu_gpu_comparison.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
    plt.show()

    # --- çµæœã®ã‚µãƒãƒªãƒ¼ ---
    # é«˜é€ŸåŒ–ç‡ã®ãƒªã‚¹ãƒˆã‚’å†è¨ˆç®—ï¼ˆå¯è¦–åŒ–ã§ã¯ä½¿ã‚ãªã‹ã£ãŸãŒã‚µãƒãƒªãƒ¼ã§ä½¿ã†ï¼‰
    speedups = [c / g for c, g in zip(cpu_times, gpu_times)]

    print("\nã€å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼ã€‘")
    print("-" * 70)
    print(
        f"æœ€å°é«˜é€ŸåŒ–ç‡: {min(speedups):.1f}x (è¡Œåˆ—ã‚µã‚¤ã‚º: {sizes[speedups.index(min(speedups))]}Ã—{sizes[speedups.index(min(speedups))]})"
    )
    print(
        f"æœ€å¤§é«˜é€ŸåŒ–ç‡: {max(speedups):.1f}x (è¡Œåˆ—ã‚µã‚¤ã‚º: {sizes[speedups.index(max(speedups))]}Ã—{sizes[speedups.index(max(speedups))]})"
    )
    print(
        "\nğŸ’¡ è€ƒå¯Ÿ: è¡Œåˆ—ã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨ãã¯GPUã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆãƒ‡ãƒ¼ã‚¿è»¢é€æ™‚é–“ç­‰ï¼‰ãŒ"
    )
    print("   ç›¸å¯¾çš„ã«å¤§ãããªã‚‹ãŸã‚ã€é«˜é€ŸåŒ–ç‡ãŒä½ããªã‚Šã¾ã™ã€‚")
    print("   è¡Œåˆ—ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã»ã©ã€GPUã®ä¸¦åˆ—è¨ˆç®—èƒ½åŠ›ãŒç™ºæ®ã•ã‚Œã€")
    print("   CPUã«å¯¾ã—ã¦åœ§å€’çš„ã«é«˜é€Ÿã«ãªã‚Šã¾ã™ã€‚")


# =====================================
# æ¼”ç¿’3: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’3: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®é–¢ä¿‚ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªã®é–¢ä¿‚ã‚’æ¨å®š
- å®Ÿéš›ã®Pre-trainedãƒ¢ãƒ‡ãƒ«ï¼ˆResNetç­‰ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¢ºèª
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3ç­‰ï¼‰ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’ç†è§£
"""
)


# --- ã‚·ãƒ³ãƒ—ãƒ«ãªå…¨çµåˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
class SimpleModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def count_parameters(model):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable


def estimate_memory(num_params, dtype="float32"):
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‹ã‚‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š"""
    bytes_per_param = {"float32": 4, "float16": 2, "int8": 1}
    bytes_size = bytes_per_param.get(dtype, 4)
    memory_bytes = num_params * bytes_size
    return memory_bytes / 1e6  # MBå˜ä½ã§è¿”ã™


# --- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦å®Ÿé¨“ ---
# configsã¯ã€Œå¤§ã€ã®ã¿ã«å¤‰æ›´
configs = [
    # ("å°", 100, 256, 10),
    # ("ä¸­", 1000, 1024, 100),
    ("å¤§", 10000, 4096, 1000),
]

print("\nã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‘")
print("-" * 70)

results = []

for name, input_size, hidden_size, output_size in configs:
    model = SimpleModel(input_size, hidden_size, output_size)
    total, trainable = count_parameters(model)

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š
    memory_params = estimate_memory(total, "float32")
    memory_with_grads = memory_params * 2  # å‹¾é…ã‚‚ä¿å­˜ã™ã‚‹ãŸã‚

    results.append(
        {
            "name": name,
            "config": f"{input_size}â†’{hidden_size}â†’{output_size}",
            "params": total,
            "memory_params": memory_params,
            "memory_grads": memory_with_grads,
        }
    )

    print(f"\nãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {name}")
    print(f"  æ§‹æˆ:             {input_size} â†’ {hidden_size} â†’ {output_size}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:     {total:,}")
    print(f"  ãƒ¡ãƒ¢ãƒª (é‡ã¿ã®ã¿): {memory_params:.2f} MB")
    print(f"  ãƒ¡ãƒ¢ãƒª (å‹¾é…è¾¼ã¿): {memory_with_grads:.2f} MB")

# --- å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Ÿé¨“ ---
print("\nã€å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Ÿé¨“ã€‘")
print("-" * 70)

if torch.cuda.is_available():
    import torchvision.models as models

    # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()

    print("ResNet50ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        resnet50 = models.resnet50(pretrained=False).to(device)

        # ãƒ€ãƒŸãƒ¼å…¥åŠ›ã§ä¸€åº¦æ¨è«–ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿ã•ã›ã‚‹
        dummy_input = torch.randn(1, 3, 224, 224).to(device)
        _ = resnet50(dummy_input)

        allocated = torch.cuda.memory_allocated() / 1e6
        reserved = torch.cuda.memory_reserved() / 1e6
        peak = torch.cuda.max_memory_allocated() / 1e6

        print(f"\nResNet50:")
        print(f"  Allocated: {allocated:.2f} MB")
        print(f"  Reserved:  {reserved:.2f} MB")
        print(f"  Peak:      {peak:.2f} MB")

        del resnet50, dummy_input
        torch.cuda.empty_cache()

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

else:
    print("GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å®Ÿéš›ã®ãƒ­ãƒ¼ãƒ‰å®Ÿé¨“ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")


# --- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªæ¨å®š ---
print("\nã€æœ‰åãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã€‘")
print("-" * 70)

large_models = {
    "ResNet-50": 25.6e6,
    "BERT-Base": 110e6,
    "GPT-2": 1.5e9,
    "GPT-3": 175e9,
    "GPT-4 (æ¨å®š)": 1.7e12,
}

print(
    f"{'ãƒ¢ãƒ‡ãƒ«å':<20} {'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°':<15} {'ãƒ¡ãƒ¢ãƒª (FP32)':<15} {'ãƒ¡ãƒ¢ãƒª (FP16)':<15}"
)
print("-" * 70)

for name, params in large_models.items():
    memory_fp32 = params * 4 / 1e9  # GB
    memory_fp16 = params * 2 / 1e9  # GB
    print(
        f"{name:<20} {params / 1e9:>10.1f}B {memory_fp32:>12.1f} GB {memory_fp16:>12.1f} GB"
    )

print("\nğŸ’¡ è€ƒå¯Ÿ:")
print("   - GPT-3ã‚¯ãƒ©ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€é‡ã¿ã ã‘ã§700GBï¼ˆFP32ï¼‰å¿…è¦")
print("   - å­¦ç¿’æ™‚ã¯å‹¾é…ã‚„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹ã‚‚ä¿å­˜ã™ã‚‹ãŸã‚ã€ã•ã‚‰ã«2-4å€å¿…è¦")
print("   - ãã®ãŸã‚ã€è¤‡æ•°ã®GPUã«åˆ†æ•£ã—ã¦é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹")
print("   - FP16ã‚„Int8é‡å­åŒ–ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›å¯èƒ½")

# --- å¯è¦–åŒ– ---
fig, ax = plt.subplots(figsize=(10, 6))

model_names = list(large_models.keys())
params_billions = [large_models[name] / 1e9 for name in model_names]
memory_fp32_gb = [p * 4 / 1e9 for p in [large_models[name] for name in model_names]]

x_pos = np.arange(len(model_names))
bars = ax.bar(x_pos, memory_fp32_gb, color="steelblue", edgecolor="black")

ax.set_xlabel("Model", fontsize=12)
ax.set_ylabel("Memory (GB, FP32)", fontsize=12)
ax.set_title(
    "Memory Requirements for Large Language Models", fontsize=14, fontweight="bold"
)
ax.set_xticks(x_pos)
ax.set_xticklabels(model_names, rotation=15, ha="right")
ax.set_yscale("log")
ax.grid(True, alpha=0.3, axis="y")

# å„æ£’ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
for i, (bar, params) in enumerate(zip(bars, params_billions)):
    height = bar.get_height()
    ax.text(
        bar.get_x() + bar.get_width() / 2.0,
        height,
        f"{params:.1f}B",
        ha="center",
        va="bottom",
        fontsize=9,
    )

plt.tight_layout()
plt.savefig("exercise3_memory_usage.png", dpi=150, bbox_inches="tight")
print("\nå›³ã‚’ 'exercise3_memory_usage.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
plt.show()

# =====================================
# æ¼”ç¿’4: CPU/GPU å­¦ç¿’æ™‚é–“ã®æ¯”è¼ƒ (MNIST)
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’4: CPU/GPU å­¦ç¿’æ™‚é–“ã®æ¯”è¼ƒ (MNIST)")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€æ‰‹æ›¸ãæ•°å­—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (MNIST) ã‚’ç”¨ã„ã¦ã€
å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å­¦ç¿’ã«ãŠã‘ã‚‹CPUã¨GPUã®é€Ÿåº¦å·®ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: ã‚·ãƒ³ãƒ—ãƒ«ãªå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ (MLP)
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: MNIST (28x28ç”»åƒ, 10ã‚¯ãƒ©ã‚¹)
"""
)

if not GPU_AVAILABLE:
    print("âš ï¸ GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€CPUã®ã¿ã®å®Ÿè¡Œã¨ãªã‚Šã¾ã™ï¼ˆæ¯”è¼ƒã§ãã¾ã›ã‚“ï¼‰")
else:
    try:
        from torchvision import datasets, transforms
        from torch.utils.data import DataLoader

        # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ---
        print("MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...")

        # å¤‰æ›å®šç¾© (Tensorå¤‰æ› + æ­£è¦åŒ–)
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ­ãƒ¼ãƒ‰
        # é€²è¡ŒçŠ¶æ³ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯True
        train_dataset = datasets.MNIST(
            root='./data',
            train=True,
            transform=transform,
            download=True
        )
        train_dataset = torch.utils.data.Subset(train_dataset, torch.randperm(len(train_dataset))[:10000])

        # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ—é–¢æ•° ---
        def train_model(device_name, dataset, batch_size=64, epochs=5):
            """æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹ã§MNISTã®å­¦ç¿’ã‚’å®Ÿè¡Œ"""
            device = torch.device(device_name)

            # DataLoaderã®ä½œæˆ
            # num_workers=2ã ã¨ç’°å¢ƒã«ã‚ˆã£ã¦ã¯ãƒãƒ³ã‚°ã™ã‚‹ãŸã‚0ã«ã™ã‚‹
            train_loader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,
                pin_memory=(device_name == "cuda")
            )

            # ãƒ¢ãƒ‡ãƒ«å®šç¾© (MNISTç”¨: 28x28=784å…¥åŠ›, 10å‡ºåŠ›)
            # Hiddenã‚µã‚¤ã‚ºã¯å°‘ã—å¤§ãã‚ã«è¨­å®šã—ã¦è¨ˆç®—è² è·ã‚’ã‹ã‘ã‚‹
            model = SimpleModel(input_size=784, hidden_size=1024, output_size=10).to(device)

            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
            criterion = nn.CrossEntropyLoss()

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— (ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è¼‰ã›ã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’åˆå›ã§æ¶ˆåŒ–)
            if device_name == "cuda":
                dummy_in = torch.randn(1, 784).to(device)
                model(dummy_in)
                torch.cuda.synchronize()

            print(f"\n[{device_name.upper()}] å­¦ç¿’é–‹å§‹ (Epochs: {epochs})...")
            start_time = time.time()

            model.train()
            for epoch in range(epochs):
                for batch_idx, (data, target) in enumerate(train_loader):
                    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã¸è»¢é€
                    data, target = data.to(device), target.to(device)

                    # ç”»åƒã‚’ãƒ•ãƒ©ãƒƒãƒˆã«ã™ã‚‹ (N, 1, 28, 28) -> (N, 784)
                    data = data.view(-1, 28*28)

                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()

            if device_name == "cuda":
                torch.cuda.synchronize()

            elapsed_time = time.time() - start_time
            print(f"[{device_name.upper()}] å®Œäº†: {elapsed_time:.4f} ç§’")

            return elapsed_time

        # --- å®Ÿé¨“è¨­å®š ---
        BATCH_SIZE = 128
        EPOCHS = 5

        print(f"è¨­å®š: Batch Size={BATCH_SIZE}, Epochs={EPOCHS}")
        print("â€» CPUã§ã®å­¦ç¿’ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")

        # CPUã§ã®å­¦ç¿’
        cpu_time = train_model("cpu", train_dataset, BATCH_SIZE, EPOCHS)

        # GPUã§ã®å­¦ç¿’
        gpu_time = train_model("cuda", train_dataset, BATCH_SIZE, EPOCHS)

        speedup = cpu_time / gpu_time
        print(f"\né«˜é€ŸåŒ–ç‡: {speedup:.2f}x")

        # --- å¯è¦–åŒ– ---
        fig, ax = plt.subplots(figsize=(8, 6))

        devices = ['CPU', 'GPU']
        times = [cpu_time, gpu_time]
        colors = ['steelblue', 'coral']

        bars = ax.bar(devices, times, color=colors)

        ax.set_ylabel('Training Time (seconds)', fontsize=12)
        ax.set_title('CPU vs GPU: MNIST Training Time', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')

        # æ™‚é–“ã‚’è¡¨ç¤º
        for bar, t in zip(bars, times):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{t:.2f}s',
                    ha='center', va='bottom', fontweight='bold')

        # é«˜é€ŸåŒ–ç‡ã‚’è¡¨ç¤º
        plt.figtext(0.5, 0.02, f"Speedup: {speedup:.1f}x", ha="center", fontsize=12, fontweight='bold', bbox={"facecolor":"orange", "alpha":0.3, "pad":5})

        plt.tight_layout()
        plt.savefig("exercise4_training_comparison.png", dpi=150, bbox_inches="tight")
        print("\nå›³ã‚’ 'exercise4_training_comparison.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
        plt.show()

        print("\nğŸ’¡ è€ƒå¯Ÿ:")
        print("   - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå­¦ç¿’ã§ã¯ã€è¨ˆç®—ã ã‘ã§ãªã")
        print("     ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯èª­è¾¼â†’CPUå‡¦ç†â†’GPUè»¢é€ï¼‰ã‚‚é‡è¦ã«ãªã‚Šã¾ã™")
        print("   - ãã‚Œã§ã‚‚GPUã®æ¼”ç®—é€Ÿåº¦ã¯åœ§å€’çš„ã§ã‚ã‚Šã€å­¦ç¿’æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã§ãã¾ã™")

    except ImportError:
        print("\nâš ï¸ torchvisionãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã“ã®æ¼”ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        print("pip install torchvision ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    except Exception as e:
        print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

print("\n" + "=" * 70)
print("æ¼”ç¿’4 å®Œäº†")
print("=" * 70)

# =====================================
# æ¼”ç¿’ã®ã¾ã¨ã‚
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’ã®ã¾ã¨ã‚")
print("=" * 70)

print(
    """
æœ¬æ¼”ç¿’ã§å­¦ã‚“ã ã“ã¨:

1. ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
   - GPUã€CPUã€ãƒ¡ãƒ¢ãƒªã®ä»•æ§˜ã‚’ç¢ºèª
   - è‡ªåˆ†ãŒä½¿ã£ã¦ã„ã‚‹è¨ˆç®—ç’°å¢ƒã‚’ç†è§£ã™ã‚‹é‡è¦æ€§

2. CPU vs GPU é€Ÿåº¦æ¯”è¼ƒ
   - GPUã¯å¤§è¦æ¨¡ãªè¡Œåˆ—æ¼”ç®—ã§åœ§å€’çš„ã«é«˜é€Ÿ
   - å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯CPUã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹
   - ãƒ‡ãƒ¼ã‚¿è»¢é€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹

3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ¯”ä¾‹ã™ã‚‹
   - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã¯è†¨å¤§ãªãƒ¡ãƒ¢ãƒªãŒå¿…è¦
   - FP16ã‚„Int8é‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›å¯èƒ½
   - å­¦ç¿’æ™‚ã¯æ¨è«–æ™‚ã®2-4å€ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦

4. CPU/GPU å­¦ç¿’æ™‚é–“ã®æ¯”è¼ƒ
   - å®Ÿéš›ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã«ãŠã„ã¦ã‚‚GPUã¯æœ‰åŠ¹
   - è¡Œåˆ—æ¼”ç®—ä¸»ä½“ã®Deep Learningã¯GPUãªã—ã§ã¯å®Ÿç”¨çš„ã§ã¯ãªã„

ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
- AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¨è«–ã«ã¯å¤§è¦æ¨¡ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦
- GPUã¯è¡Œåˆ—æ¼”ç®—ã«ç‰¹åŒ–ã—ãŸä¸¦åˆ—è¨ˆç®—è£…ç½®
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã¯æ•°åƒï½æ•°ä¸‡ã®GPUã‚’ä½¿ç”¨
- ãƒ¡ãƒ¢ãƒªã‚‚CPUã¨åŒæ§˜ã«é‡è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ç†è§£ã¯AIé–‹ç™ºã«ä¸å¯æ¬ 

"""
)
