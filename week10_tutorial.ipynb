{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297d3763",
   "metadata": {},
   "source": [
    "# MDA入門 2025年度 第10回演習\n",
    "## 大規模言語モデル (LLM) とプロンプティング\n",
    "\n",
    "大規模な言語モデル（LLM）の挙動を理解し，効果的に活用するための技術であるプロンプトエンジニアリングの基礎を学びます．\n",
    "\n",
    "**演習内容:**\n",
    "1. プロンプティングの実践（Zero-shot / Few-shot）\n",
    "2. In-Context Learningの体験（ラベルのランダム化や順序の影響）\n",
    "3. Chain-of-Thought (CoT) Promptingの実験\n",
    "\n",
    "**使用モデル:**\n",
    "本演習では，Google Colabの無償枠（T4 GPU）でも動作する軽量かつ高性能なモデル（Qwen2.5-0.5B-Instructなど）を使用します．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02244be",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップ\n",
    "\n",
    "演習に必要なライブラリをインストールし，環境を整えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff2183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "セットアップ完了\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install -q transformers torch datasets accelerate sentencepiece\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 日本語フォントの設定\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"DejaVu Sans\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# シード固定（再現性のため）\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"セットアップ完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fdb500",
   "metadata": {},
   "source": [
    "## 2. LLMモデルの読み込み\n",
    "\n",
    "Colab上で十分に動作するモデルをいくつか定義し，利用可能なものを読み込みます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c235db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cuda\n",
      "\n",
      "試行 1/4: Qwen/Qwen2.5-0.5B-Instruct\n",
      "  パラメータ数: 500M\n",
      "  読み込み中..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [OK]\n",
      "\n",
      "テキスト生成パイプラインの準備完了\n"
     ]
    }
   ],
   "source": [
    "# 試行するモデルのリスト（優先順位順）\n",
    "model_candidates = [\n",
    "    {\n",
    "        \"name\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"params\": \"500M\",\n",
    "        \"description\": \"Alibaba開発，多言語対応，Instructionチューニング済み\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"llm-jp/llm-jp-3-1.8b-instruct\",\n",
    "        \"params\": \"1.8B\",\n",
    "        \"description\": \"国立情報学研究所開発，日本語特化\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"rinna/japanese-gpt2-medium\",\n",
    "        \"params\": \"330M\",\n",
    "        \"description\": \"日本語GPT-2，実績豊富\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cyberagent/open-calm-small\",\n",
    "        \"params\": \"160M\",\n",
    "        \"description\": \"サイバーエージェント開発，超軽量\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# デバイスの確認\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# モデルのロード試行\n",
    "model = None\n",
    "tokenizer = None\n",
    "model_name = None\n",
    "\n",
    "for i, candidate in enumerate(model_candidates, 1):\n",
    "    model_name = candidate[\"name\"]\n",
    "    print(f\"\\n試行 {i}/4: {model_name}\")\n",
    "    print(f\"  パラメータ数: {candidate['params']}\")\n",
    "    print(\"  読み込み中...\", end=\"\")\n",
    "\n",
    "    try:\n",
    "        # トークナイザーの読み込み\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # モデルの読み込み（メモリ効率化）\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        print(\" [OK]\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\" [NG]\")\n",
    "        print(f\"  エラー: {str(e)[:100]}...\")\n",
    "\n",
    "        # メモリをクリア\n",
    "        if model is not None:\n",
    "            del model\n",
    "        if tokenizer is not None:\n",
    "            del tokenizer\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "\n",
    "if model is None:\n",
    "    raise RuntimeError(\"すべてのモデルのロードに失敗しました．\")\n",
    "\n",
    "# テキスト生成用のパイプライン作成\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nテキスト生成パイプラインの準備完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b631b4",
   "metadata": {},
   "source": [
    "## 3. データセットの準備\n",
    "\n",
    "演習で使用するサンプルデータを作成します．Amazon等の商品レビューを模したテキストと，その感情ラベル（positive/negative/neutral）のペアを用意します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e4d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータ作成完了（10件）\n",
      "【データ例】\n",
      "{'text': 'この商品は最高です！買って良かったです．', 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "dataset = [\n",
    "    {\"text\": \"この商品は最高です！買って良かったです．\", \"label\": \"positive\"},\n",
    "    {\"text\": \"期待外れでした．品質が悪すぎます．\", \"label\": \"negative\"},\n",
    "    {\"text\": \"値段の割には普通です．可もなく不可もなく．\", \"label\": \"neutral\"},\n",
    "    {\"text\": \"素晴らしい製品です．友人にも勧めたいと思います．\", \"label\": \"positive\"},\n",
    "    {\"text\": \"サービスが最悪でした．二度と利用しません．\", \"label\": \"negative\"},\n",
    "    {\"text\": \"機能は良いですが，デザインがイマイチです．\", \"label\": \"neutral\"},\n",
    "    {\"text\": \"期待以上の性能でした．大満足です！\", \"label\": \"positive\"},\n",
    "    {\"text\": \"説明と実物が全然違う．詐欺レベルです．\", \"label\": \"negative\"},\n",
    "    {\"text\": \"この価格なら妥当だと思います．\", \"label\": \"neutral\"},\n",
    "    {\"text\": \"感動しました．本当に買って良かったです．\", \"label\": \"positive\"},\n",
    "]\n",
    "\n",
    "print(f\"サンプルデータ作成完了（{len(dataset)}件）\")\n",
    "print(\"【データ例】\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a51957",
   "metadata": {},
   "source": [
    "## 4. 演習1: プロンプティングの基礎実践 (Zero-shot)\n",
    "\n",
    "**Zero-shot prompting**とは，LLMに対して例題を与えずに，直接タスクの指示のみを行って回答を生成させる手法です．\n",
    "まずはこちらを試してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9fce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト: この商品は最高です！買って良かったです．\n",
      "正解ラベル: positive\n",
      "--------------------------------------------------\n",
      "プロンプト：\n",
      "以下のテキストの感情を「ポジティブ」「ネガティブ」「中立」のいずれかで分類してください．\n",
      "\n",
      "テキスト: この商品は最高です！買って良かったです．\n",
      "\n",
      "感情:\n",
      "--------------------------------------------------\n",
      "モデルの回答: 感情: 感情: 中立\n",
      "\n",
      "理由：このテキストは\n"
     ]
    }
   ],
   "source": [
    "# テスト用のテキストを選択\n",
    "test_text = dataset[0][\"text\"]\n",
    "test_label = dataset[0][\"label\"]\n",
    "\n",
    "print(f\"テキスト: {test_text}\")\n",
    "print(f\"正解ラベル: {test_label}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Zero-shot プロンプトの作成\n",
    "zero_shot_prompt = f\"\"\"以下のテキストの感情を「ポジティブ」「ネガティブ」「中立」のいずれかで分類してください．\n",
    "\n",
    "テキスト: {test_text}\n",
    "\n",
    "感情:\"\"\"\n",
    "\n",
    "print(\"プロンプト：\")\n",
    "print(zero_shot_prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# テキスト生成\n",
    "output = generator(\n",
    "    zero_shot_prompt,\n",
    "    max_new_tokens=20,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "# 入力プロンプトを除いた生成部分のみを抽出\n",
    "result = generated_text[len(zero_shot_prompt):].strip()\n",
    "\n",
    "print(f\"モデルの回答: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2680da1",
   "metadata": {},
   "source": [
    "### 【考察】\n",
    "実行結果を見て，Zero-shotプロンプティングの特徴や課題について考えてみましょう．正しく分類できましたか？また，余計な出力が含まれていたりしませんか？\n",
    "\n",
    "（ここに考察を記入してください）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb84c34",
   "metadata": {},
   "source": [
    "## 5. 演習2: In-Context Learning (ICL) の実験\n",
    "\n",
    "**Few-shot prompting**（In-Context Learning）では，プロンプトの中にいくつかの例題（Example）を含めることで，モデルにタスクの形式やパターンを学習させます．\n",
    "\n",
    "ここでは以下の3つの条件で実験を行い，結果を比較します：\n",
    "1. **標準的なFew-shot**: 正解ラベルを使用した通常のFew-shot\n",
    "2. **ラベルランダム化**: 例題のラベルをデタラメに入れ替えたもの\n",
    "3. **順序変更**: 例題の提示順序をシャッフルしたもの\n",
    "\n",
    "これにより，モデルが「ラベルの意味」を理解しているのか，それとも「形式」を重視しているのかといった性質を探ります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4b9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データ数: 5件\n",
      "------------------------------------------------------------\n",
      "\n",
      "◆テストケース 1: 機能は良いですが，デザインがイマイチです．... (正解: neutral)\n",
      "    - 標準Few-shot: negative [NG]\n",
      "    - ラベルランダム: neutral [OK]\n",
      "    - 順序変更: negative [NG]\n",
      "\n",
      "◆テストケース 2: 期待以上の性能でした．大満足です！... (正解: positive)\n",
      "    - 標準Few-shot: positive [OK]\n",
      "    - ラベルランダム: positive [OK]\n",
      "    - 順序変更: positive [OK]\n",
      "\n",
      "◆テストケース 3: 説明と実物が全然違う．詐欺レベルです．... (正解: negative)\n",
      "    - 標準Few-shot: negative [OK]\n",
      "    - ラベルランダム: negative [OK]\n",
      "    - 順序変更: negative [OK]\n",
      "\n",
      "◆テストケース 4: この価格なら妥当だと思います．... (正解: neutral)\n",
      "    - 標準Few-shot: neutral [OK]\n",
      "    - ラベルランダム: neutral [OK]\n",
      "    - 順序変更: neutral [OK]\n",
      "\n",
      "◆テストケース 5: 感動しました．本当に買って良かったです．... (正解: positive)\n",
      "    - 標準Few-shot: positive [OK]\n",
      "    - ラベルランダム: positive [OK]\n",
      "    - 順序変更: positive [OK]\n",
      "\n",
      "============================================================\n",
      "【実験結果サマリ（正解率）】\n",
      "標準Few-shot: 80.0%\n",
      "ラベルランダム: 100.0%\n",
      "順序変更: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# ICL用のヘルパー関数定義\n",
    "\n",
    "def generate_prediction(prompt, generator, tokenizer):\n",
    "    try:\n",
    "        output = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=5, # 短い回答のみ取得\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "            truncation=True\n",
    "        )\n",
    "        generated = output[0][\"generated_text\"][len(prompt):].strip()\n",
    "        return generated.split('\\n')[0].strip()\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "def build_prompt(examples, target_text, randomize_labels=False):\n",
    "    prompt = \"以下のテキストの感情を「ポジティブ」「ネガティブ」「中立」で分類してください．\\n\\n\"\n",
    "    labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "    for i, ex in enumerate(examples):\n",
    "        txt = ex[\"text\"]\n",
    "        lbl = ex[\"label\"]\n",
    "\n",
    "        if randomize_labels:\n",
    "            lbl = random.choice(labels)\n",
    "\n",
    "        prompt += f\"例{i+1}:\\nテキスト: {txt}\\n感情: {lbl}\\n\\n\"\n",
    "\n",
    "    prompt += f\"テキスト: {target_text}\\n感情:\"\n",
    "    return prompt\n",
    "\n",
    "# --------------------\n",
    "# 実験実行\n",
    "# --------------------\n",
    "\n",
    "n_shots = 5\n",
    "icl_examples = dataset[:n_shots]      # 例示用データ\n",
    "icl_test_data = dataset[n_shots:n_shots+5] # テスト用データ\n",
    "\n",
    "conditions = [\n",
    "    {\"name\": \"標準Few-shot\", \"random_label\": False, \"shuffle_order\": False},\n",
    "    {\"name\": \"ラベルランダム\", \"random_label\": True, \"shuffle_order\": False},\n",
    "    {\"name\": \"順序変更\",     \"random_label\": False, \"shuffle_order\": True},\n",
    "]\n",
    "\n",
    "results = {c[\"name\"]: [] for c in conditions}\n",
    "\n",
    "print(f\"評価データ数: {len(icl_test_data)}件\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, sample in enumerate(icl_test_data):\n",
    "    test_text = sample[\"text\"]\n",
    "    true_label = sample[\"label\"]\n",
    "\n",
    "    print(f\"\\n◆テストケース {idx+1}: {test_text[:30]}... (正解: {true_label})\")\n",
    "\n",
    "    for cond in conditions:\n",
    "        current_examples = list(icl_examples)\n",
    "\n",
    "        if cond[\"shuffle_order\"]:\n",
    "            random.shuffle(current_examples)\n",
    "\n",
    "        prompt = build_prompt(\n",
    "            current_examples,\n",
    "            test_text,\n",
    "            randomize_labels=cond[\"random_label\"]\n",
    "        )\n",
    "\n",
    "        pred = generate_prediction(prompt, generator, tokenizer)\n",
    "        is_correct = (true_label in pred)\n",
    "        results[cond[\"name\"]].append(is_correct)\n",
    "\n",
    "        mark = \"[OK]\" if is_correct else \"[NG]\"\n",
    "        print(f\"    - {cond['name']}: {pred} {mark}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【実験結果サマリ（正解率）】\")\n",
    "for name, res in results.items():\n",
    "    if len(res) > 0:\n",
    "        acc = sum(res) / len(res) * 100\n",
    "        print(f\"{name}: {acc:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{name}: データなし\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297cc37",
   "metadata": {},
   "source": [
    "### 【考察】\n",
    "1. Few-shotとZero-shotの結果を比較して，どのような変化がありましたか．\n",
    "2. ラベルをランダムに入れ替えた場合と正しいラベルの場合で，結果にどのような違い（あるいは共通点）が見られましたか．そこからICLの性質について何が言えますか．\n",
    "\n",
    "（ここに考察を記入してください）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0a2c2",
   "metadata": {},
   "source": [
    "## 6. 演習3: Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "**Chain-of-Thought (CoT)** は，モデルに「段階的に考える」ことを促す手法です．推論過程（Reasoning）を明示的に出力させることで，複雑な問題（算数や論理問題など）の正答率を向上させることができます．\n",
    "\n",
    "ここでは算数の文章題を用いて，以下の3パターンを比較します：\n",
    "1. **通常Few-shot**: 答えのみを例示\n",
    "2. **CoT Few-shot**: 例示の中に「考え方」を含める\n",
    "3. **Zero-shot CoT**: 例示なしで「段階的に考えましょう」とだけ指示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec5c36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "問題: 本屋で500円の本を2冊と300円の雑誌を1冊買いました．合計金額はいくらですか？\n",
      "正解: 1300円\n",
      "--------------------------------------------------\n",
      "\n",
      "【通常Few-shot】\n",
      "出力: 700円です。\n",
      "\n",
      "問題: 食べ物店で100円のチーズを1本买了。また、そのチーズを10本分切って、100円の小鉢を作りました。食料品の価格はどれ...\n",
      "\n",
      "【CoT Few-shot】\n",
      "出力: まず、500円の本を2冊买了なので，500 × 2 = 1000円でした。さらに、1冊の雑誌が300円でしたので，1000 + 300 = 13...\n",
      "\n",
      "【Zero-shot CoT】\n",
      "出力: まず、本屋で購入した物の価格を計算します。500円の本を2冊买了り、300円の雑誌を1冊买了りすると、それぞれの価格が同じです。\n",
      "\n",
      "本の価格 = 5...\n"
     ]
    }
   ],
   "source": [
    "# 問題定義\n",
    "test_problem = {\n",
    "    \"question\": \"本屋で500円の本を2冊と300円の雑誌を1冊買いました．合計金額はいくらですか？\",\n",
    "    \"answer\": \"1300円\",\n",
    "}\n",
    "\n",
    "math_problems = [\n",
    "    {\n",
    "        \"question\": \"太郎は5個のりんごを持っていました．3個もらいました．今何個ありますか？\",\n",
    "        \"answer\": \"8個\",\n",
    "        \"reasoning\": \"最初に5個持っていて，3個もらったので，5 + 3 = 8個です．\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"花子は10個のみかんを持っていました．4個食べました．今何個ありますか？\",\n",
    "        \"answer\": \"6個\",\n",
    "        \"reasoning\": \"最初に10個持っていて，4個食べたので，10 - 4 = 6個です．\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"問題: {test_problem['question']}\")\n",
    "print(f\"正解: {test_problem['answer']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 1. 通常Few-shot\n",
    "standard_prompt = \"以下の算数の問題を解いてください．\\n\\n\"\n",
    "for ex in math_problems:\n",
    "    standard_prompt += f\"問題: {ex['question']}\\n答え: {ex['answer']}\\n\\n\"\n",
    "standard_prompt += f\"問題: {test_problem['question']}\\n答え:\"\n",
    "\n",
    "# 2. CoT Few-shot\n",
    "cot_prompt = \"以下の算数の問題を，計算過程を示しながら解いてください．\\n\\n\"\n",
    "for ex in math_problems:\n",
    "    cot_prompt += f\"問題: {ex['question']}\\n考え方: {ex['reasoning']}\\n答え: {ex['answer']}\\n\\n\"\n",
    "cot_prompt += f\"問題: {test_problem['question']}\\n考え方:\"\n",
    "\n",
    "# 3. Zero-shot CoT\n",
    "zero_shot_cot_prompt = f\"\"\"以下の算数の問題を解いてください．段階的に考えましょう．\n",
    "\n",
    "問題: {test_problem['question']}\n",
    "\n",
    "考え方:\"\"\"\n",
    "\n",
    "# 各プロンプトでの実行\n",
    "prompts = {\n",
    "    \"通常Few-shot\": standard_prompt,\n",
    "    \"CoT Few-shot\": cot_prompt,\n",
    "    \"Zero-shot CoT\": zero_shot_cot_prompt\n",
    "}\n",
    "\n",
    "for name, prompt in prompts.items():\n",
    "    print(f\"\\n【{name}】\")\n",
    "    try:\n",
    "        out = generator(\n",
    "            prompt,\n",
    "            max_new_tokens=60,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "        )\n",
    "        res = out[0][\"generated_text\"][len(prompt):].strip()\n",
    "        print(f\"出力: {res[:100]}...\")\n",
    "    except:\n",
    "        print(\"エラーが発生しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa796f",
   "metadata": {},
   "source": [
    "### 【考察】\n",
    "推論過程（考え方）をプロンプトに含めることで，モデルの出力はどう変化しましたか．単に答えが合うかどうかだけでなく，その過程が論理的かどうかも確認してください．\n",
    "\n",
    "（ここに考察を記入してください）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb55d7",
   "metadata": {},
   "source": [
    "## 演習まとめ\n",
    "\n",
    "本演習を通じて，以下のことが確認できました．\n",
    "\n",
    "1. **プロンプト設計の重要性**: 指示の仕方一つでモデルの出力精度が大きく変わること．\n",
    "2. **ICLの特性**: LLMは例題から「タスクの形式」を強く学習し，ラベルの意味よりも形式的な整合性を重視する場合があること（Label Chaos）．\n",
    "3. **CoTの効果**: 計算などの推論タスクでは，思考過程を言語化させること（CoT）で性能が向上すること．\n",
    "\n",
    "### 参考文献\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Thinking about prompting (Min et al., 2022)](https://arxiv.org/abs/2202.12837)\n",
    "- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)](https://arxiv.org/abs/2201.11903)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
