"""
MDAå…¥é–€ 2025å¹´åº¦ ç¬¬10å›æ¼”ç¿’
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°

æ¼”ç¿’å†…å®¹:
1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®å®Ÿè·µï¼ˆZero-shot / Few-shotï¼‰
2. In-Context Learningã®ä½“é¨“
3. Chain-of-Thought (CoT) Promptingã®å®Ÿé¨“

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:
- rinna/japanese-gpt2-medium (æ—¥æœ¬èªGPT-2ã€APIã‚­ãƒ¼ä¸è¦)
- cyberagent/open-calm-small (æ—¥æœ¬èªLLMã€APIã‚­ãƒ¼ä¸è¦)

ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
- livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ (å…¬é–‹æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿)
- JGLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (æ—¥æœ¬èªè‡ªç„¶è¨€èªç†è§£ã‚¿ã‚¹ã‚¯)
"""

# ============================================
# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ============================================

print("=" * 70)
print("ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
print("=" * 70)

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
#!pip install -q transformers torch datasets sentencepiece fugashi ipadic

import random
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

warnings.filterwarnings("ignore")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


# ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


set_seed(42)

print("\nâœ“ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
print("=" * 70)
print("MDAå…¥é–€ ç¬¬10å›æ¼”ç¿’: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°")
print("=" * 70)

# ============================================
# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
# ============================================

print("\n" + "=" * 70)
print("å…¬é–‹LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
print("=" * 70)

# è»½é‡ãªæ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰
model_name = "rinna/japanese-gpt2-medium"  # ç´„330Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

print(f"\nãƒ¢ãƒ‡ãƒ«: {model_name}")
print("â€» ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ç„¡æ–™ã§åˆ©ç”¨å¯èƒ½ãªå…¬é–‹ãƒ¢ãƒ‡ãƒ«ã§ã™ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰")
print("èª­ã¿è¾¼ã¿ä¸­...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã‚’ä½¿ç”¨
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    print("âœ“ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"  ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ç´„{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M")

except Exception as e:
    print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
    print("åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™...")
    model_name = "cyberagent/open-calm-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    print(f"âœ“ ä»£æ›¿ãƒ¢ãƒ‡ãƒ« {model_name} ã®èª­ã¿è¾¼ã¿å®Œäº†")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if device == "cuda" else -1,
)

print("\nâœ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™å®Œäº†")

# ============================================
# å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
# ============================================

print("\n" + "=" * 70)
print("å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿")
print("=" * 70)

print("\nlivedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™")
print("â€» æ—¥æœ¬èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ï¼ˆ9ã‚«ãƒ†ã‚´ãƒªï¼‰ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

try:
    # livedoor ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã®èª­ã¿è¾¼ã¿
    dataset = load_dataset("livedoor_news_corpus", split="train")

    print("âœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
    print(f"  ç·è¨˜äº‹æ•°: {len(dataset)}")
    print(f"  ã‚«ãƒ†ã‚´ãƒª: {set(dataset['category'])}")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
    sample = dataset[0]
    print("\nã€ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹ã€‘")
    print(f"ã‚«ãƒ†ã‚´ãƒª: {sample['category']}")
    print(f"ã‚¿ã‚¤ãƒˆãƒ«: {sample['title']}")
    print(f"æœ¬æ–‡ï¼ˆæŠœç²‹ï¼‰: {sample['text'][:100]}...")

except Exception as e:
    print(f"\nâš ï¸ livedoorãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    print("ä»£æ›¿ã¨ã—ã¦æ‰‹å‹•ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™...")

    # ä»£æ›¿ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    dataset = [
        {
            "category": "sports",
            "title": "æ—¥æœ¬ä»£è¡¨ãŒåŠ‡çš„å‹åˆ©",
            "text": "ã‚µãƒƒã‚«ãƒ¼æ—¥æœ¬ä»£è¡¨ã¯æ˜¨æ—¥ã®è©¦åˆã§3-2ã®é€†è»¢å‹åˆ©ã‚’åã‚ãŸã€‚å¾ŒåŠãƒ­ã‚¹ã‚¿ã‚¤ãƒ ã®æ±ºå‹ã‚´ãƒ¼ãƒ«ã§ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ã¯å¤§æ­“å£°ã«åŒ…ã¾ã‚ŒãŸã€‚",
        },
        {
            "category": "technology",
            "title": "æ–°å‹AIãƒãƒƒãƒ—ç™ºè¡¨",
            "text": "å¤§æ‰‹åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ãŒæ¬¡ä¸–ä»£AIãƒãƒƒãƒ—ã‚’ç™ºè¡¨ã—ãŸã€‚æ€§èƒ½ã¯å‰ä¸–ä»£æ¯”ã§5å€ã«å‘ä¸Šã—ã€æ¶ˆè²»é›»åŠ›ã¯åŠåˆ†ã«å‰Šæ¸›ã•ã‚Œã¦ã„ã‚‹ã€‚",
        },
        {
            "category": "entertainment",
            "title": "äººæ°—æ˜ ç”»ãŒèˆˆè¡Œåå…¥è¨˜éŒ²æ›´æ–°",
            "text": "å…¬é–‹ä¸­ã®è©±é¡Œä½œãŒé€±æœ«èˆˆè¡Œåå…¥ã§æ­´ä»£æœ€é«˜è¨˜éŒ²ã‚’æ›´æ–°ã—ãŸã€‚å…¨å›½ã®æ˜ ç”»é¤¨ã§æº€å¸­ãŒç¶šå‡ºã—ã¦ã„ã‚‹ã€‚",
        },
        {
            "category": "economy",
            "title": "æ ªä¾¡ãŒæ€¥ä¸Šæ˜‡",
            "text": "æ±äº¬æ ªå¼å¸‚å ´ã§æ—¥çµŒå¹³å‡æ ªä¾¡ãŒå¤§å¹…ã«ä¸Šæ˜‡ã—ãŸã€‚å¥½èª¿ãªä¼æ¥­æ¥­ç¸¾ã‚’å—ã‘ã¦æŠ•è³‡å®¶å¿ƒç†ãŒæ”¹å–„ã—ã¦ã„ã‚‹ã€‚",
        },
        {
            "category": "technology",
            "title": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§æ–°è¨˜éŒ²",
            "text": "ç ”ç©¶ãƒãƒ¼ãƒ ãŒé‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸè¨ˆç®—ã§æ–°è¨˜éŒ²ã‚’é”æˆã—ãŸã€‚å¾“æ¥ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§ã¯ä¸å¯èƒ½ã ã£ãŸè¦æ¨¡ã®è¨ˆç®—ã«æˆåŠŸã—ãŸã€‚",
        },
    ]
    print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†ï¼ˆ{len(dataset)}ä»¶ï¼‰")

# ============================================
# æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ")
print("=" * 70)

print("""
ã“ã®æ¼”ç¿’ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆãŒå‡ºåŠ›ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’å­¦ã³ã¾ã™ã€‚
- Zero-shot promptingï¼ˆä¾‹é¡Œãªã—ï¼‰
- Few-shot promptingï¼ˆä¾‹é¡Œã‚ã‚Šï¼‰
ã®é•ã„ã‚’å®Ÿéš›ã«ç¢ºèªã—ã¾ã™ã€‚
""")

# --- ã‚¿ã‚¹ã‚¯1: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„ ---
print("\n" + "-" * 70)
print("ã‚¿ã‚¹ã‚¯1: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç´„")
print("-" * 70)

# ãƒ†ã‚¹ãƒˆç”¨ã®è¨˜äº‹ã‚’é¸æŠ
if isinstance(dataset, list):
    test_article = dataset[0]
else:
    test_article = dataset[100]

print("\nã€å…ƒè¨˜äº‹ã€‘")
print(f"ã‚«ãƒ†ã‚´ãƒª: {test_article['category']}")
print(f"ã‚¿ã‚¤ãƒˆãƒ«: {test_article['title']}")
if isinstance(dataset, list):
    print(f"æœ¬æ–‡: {test_article['text']}")
else:
    print(f"æœ¬æ–‡: {test_article['text'][:200]}...")

# Zero-shot prompting
print("\nã€å®Ÿé¨“1-1ã€‘Zero-shot Prompting")
print("-" * 50)

zero_shot_prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’1æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹: {test_article["text"][:200] if not isinstance(dataset, list) else test_article["text"]}

è¦ç´„:"""

print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
print(zero_shot_prompt[:150] + "...")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
try:
    output_zero = generator(
        zero_shot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )

    generated_text = output_zero[0]["generated_text"]
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»ã—ã¦ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
    summary_zero = generated_text[len(zero_shot_prompt) :].strip()

    print("\nç”Ÿæˆã•ã‚ŒãŸè¦ç´„:")
    print(f"{summary_zero[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    summary_zero = "ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"

# Few-shot prompting
print("\n\nã€å®Ÿé¨“1-2ã€‘Few-shot Promptingï¼ˆä¾‹é¡Œä»˜ãï¼‰")
print("-" * 50)

# ä¾‹é¡Œã‚’ä½œæˆ
if isinstance(dataset, list):
    examples = dataset[1:3]
else:
    examples = [dataset[i] for i in [10, 20]]

few_shot_prompt = "ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’1æ–‡ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œã‚’è¿½åŠ 
for i, example in enumerate(examples, 1):
    ex_text = (
        example["text"][:150] if not isinstance(dataset, list) else example["text"]
    )
    few_shot_prompt += f"ä¾‹{i}:\nè¨˜äº‹: {ex_text}\nè¦ç´„: {example['title']}\n\n"

# ãƒ†ã‚¹ãƒˆè¨˜äº‹ã‚’è¿½åŠ 
test_text = (
    test_article["text"][:200]
    if not isinstance(dataset, list)
    else test_article["text"]
)
few_shot_prompt += f"è¨˜äº‹: {test_text}\n\nè¦ç´„:"

print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¾‹é¡Œ2ã¤ä»˜ãï¼‰:")
print(few_shot_prompt[:200] + "...")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
try:
    output_few = generator(
        few_shot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )

    generated_text_few = output_few[0]["generated_text"]
    summary_few = generated_text_few[len(few_shot_prompt) :].strip()

    print("\nç”Ÿæˆã•ã‚ŒãŸè¦ç´„:")
    print(f"{summary_few[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    summary_few = "ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"

# æ¯”è¼ƒ
print("\n" + "=" * 70)
print("ã€æ¯”è¼ƒçµæœã€‘")
print("=" * 70)
print(f"Zero-shot: {summary_zero[:80]}...")
print(f"Few-shot:  {summary_few[:80]}...")
print("\nğŸ’¡ Few-shotã§ã¯ä¾‹é¡Œã‹ã‚‰å½¢å¼ã‚„é•·ã•ã‚’å­¦ç¿’ã—ã€ã‚ˆã‚Šé©åˆ‡ãªè¦ç´„ã«ãªã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™")

# ============================================
# æ¼”ç¿’2: In-Context Learningã®ä½“é¨“
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’2: In-Context Learningã®ä½“é¨“")
print("=" * 70)

print("""
ä¾‹é¡Œã®æ•°ã‚’å¤‰ãˆã‚‹ã“ã¨ã§æ€§èƒ½ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
ã‚¿ã‚¹ã‚¯: ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
""")

# --- ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
print("\n" + "-" * 70)
print("ãƒ‡ãƒ¼ã‚¿æº–å‚™")
print("-" * 70)

# ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
if isinstance(dataset, list):
    categories = list(set([item["category"] for item in dataset]))
    category_samples = {}
    for cat in categories:
        category_samples[cat] = [item for item in dataset if item["category"] == cat]
else:
    categories = list(set(dataset["category"]))
    category_samples = {}
    for cat in categories[:5]:  # æœ€åˆã®5ã‚«ãƒ†ã‚´ãƒªã®ã¿ä½¿ç”¨
        samples = [item for item in dataset if item["category"] == cat]
        category_samples[cat] = samples[:10]  # å„ã‚«ãƒ†ã‚´ãƒª10ä»¶ã¾ã§

print(f"ä½¿ç”¨ã‚«ãƒ†ã‚´ãƒª: {list(category_samples.keys())}")
print(f"å„ã‚«ãƒ†ã‚´ãƒªã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {[len(v) for v in category_samples.values()]}")

# ãƒ†ã‚¹ãƒˆç”¨è¨˜äº‹ã‚’é¸æŠ
test_category = list(category_samples.keys())[0]
if isinstance(dataset, list):
    test_sample = category_samples[test_category][0]
else:
    test_sample = category_samples[test_category][-1]

print("\nã€ãƒ†ã‚¹ãƒˆè¨˜äº‹ã€‘")
print(f"æ­£è§£ã‚«ãƒ†ã‚´ãƒª: {test_sample['category']}")
print(f"ã‚¿ã‚¤ãƒˆãƒ«: {test_sample['title']}")

# --- å®Ÿé¨“: ä¾‹é¡Œæ•°ã‚’å¤‰ãˆã¦åˆ†é¡ ---
print("\n" + "-" * 70)
print("å®Ÿé¨“: ä¾‹é¡Œæ•°ã¨åˆ†é¡ç²¾åº¦ã®é–¢ä¿‚")
print("-" * 70)


def create_classification_prompt(num_examples, test_text, category_samples):
    """åˆ†é¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    prompt = "ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n"
    prompt += f"ã‚«ãƒ†ã‚´ãƒª: {', '.join(category_samples.keys())}\n\n"

    if num_examples > 0:
        # ä¾‹é¡Œã‚’è¿½åŠ 
        examples_added = 0
        for cat, samples in category_samples.items():
            for sample in samples[:num_examples]:
                if examples_added >= num_examples * len(category_samples):
                    break
                ex_text = sample["text"][:100] if "text" in sample else sample["title"]
                prompt += f"è¨˜äº‹: {ex_text}...\nã‚«ãƒ†ã‚´ãƒª: {sample['category']}\n\n"
                examples_added += 1

    # ãƒ†ã‚¹ãƒˆè¨˜äº‹ã‚’è¿½åŠ 
    prompt += f"è¨˜äº‹: {test_text}...\nã‚«ãƒ†ã‚´ãƒª:"
    return prompt


# Zero-shotï¼ˆä¾‹é¡Œ0å€‹ï¼‰
print("\nã€Zero-shotï¼ˆä¾‹é¡Œãªã—ï¼‰ã€‘")
test_text = test_sample["text"][:100] if "text" in test_sample else test_sample["title"]
prompt_0shot = create_classification_prompt(0, test_text, category_samples)

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt_0shot[:200]}...")

try:
    output = generator(
        prompt_0shot,
        max_new_tokens=10,
        num_return_sequences=1,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )
    result_0shot = output[0]["generated_text"][len(prompt_0shot) :].strip()
    print(f"äºˆæ¸¬ã‚«ãƒ†ã‚´ãƒª: {result_0shot[:20]}")
except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_0shot = "ã‚¨ãƒ©ãƒ¼"

# Few-shotï¼ˆä¾‹é¡Œ2å€‹ï¼‰
print("\nã€Few-shotï¼ˆä¾‹é¡Œ2å€‹ãšã¤ï¼‰ã€‘")
prompt_2shot = create_classification_prompt(2, test_text, category_samples)

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¾‹é¡Œã‚ã‚Šï¼‰:\n{prompt_2shot[:250]}...")

try:
    output = generator(
        prompt_2shot,
        max_new_tokens=10,
        num_return_sequences=1,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )
    result_2shot = output[0]["generated_text"][len(prompt_2shot) :].strip()
    print(f"äºˆæ¸¬ã‚«ãƒ†ã‚´ãƒª: {result_2shot[:20]}")
except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_2shot = "ã‚¨ãƒ©ãƒ¼"

print("\n" + "=" * 70)
print("ã€çµæœæ¯”è¼ƒã€‘")
print("=" * 70)
print(f"æ­£è§£ã‚«ãƒ†ã‚´ãƒª:  {test_sample['category']}")
print(f"Zero-shotäºˆæ¸¬: {result_0shot[:20]}")
print(f"Few-shotäºˆæ¸¬:  {result_2shot[:20]}")
print("\nğŸ’¡ ä¾‹é¡ŒãŒã‚ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚¿ã‚¹ã‚¯ã®å½¢å¼ã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç†è§£ã§ãã¾ã™")

# ============================================
# æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting")
print("=" * 70)

print("""
æ€è€ƒéç¨‹ã‚’ä¾‹é¡Œã«å«ã‚ã‚‹ã“ã¨ã§ã€æ¨è«–èƒ½åŠ›ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
ã‚¿ã‚¹ã‚¯: ç°¡å˜ãªç®—æ•°ã®æ–‡ç« é¡Œ
""")

# --- ç®—æ•°æ–‡ç« é¡Œã®ã‚µãƒ³ãƒ—ãƒ« ---
math_problems = [
    {
        "question": "å¤ªéƒã¯5å€‹ã®ã‚Šã‚“ã”ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚3å€‹ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "answer": "8å€‹",
        "reasoning": "æœ€åˆã«5å€‹æŒã£ã¦ã„ã¦ã€3å€‹ã‚‚ã‚‰ã£ãŸã®ã§ã€5 + 3 = 8å€‹ã§ã™ã€‚",
    },
    {
        "question": "èŠ±å­ã¯10å€‹ã®ã¿ã‹ã‚“ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚4å€‹é£Ÿã¹ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "answer": "6å€‹",
        "reasoning": "æœ€åˆã«10å€‹æŒã£ã¦ã„ã¦ã€4å€‹é£Ÿã¹ãŸã®ã§ã€10 - 4 = 6å€‹ã§ã™ã€‚",
    },
    {
        "question": "ã‚¯ãƒ©ã‚¹ã«30äººã®ç”Ÿå¾’ãŒã„ã¾ã™ã€‚ãã®ã†ã¡12äººãŒç”·å­ã§ã™ã€‚å¥³å­ã¯ä½•äººã§ã™ã‹ï¼Ÿ",
        "answer": "18äºº",
        "reasoning": "å…¨ä½“ãŒ30äººã§ã€ç”·å­ãŒ12äººãªã®ã§ã€å¥³å­ã¯ 30 - 12 = 18äººã§ã™ã€‚",
    },
]

# ãƒ†ã‚¹ãƒˆå•é¡Œ
test_problem = {
    "question": "æœ¬å±‹ã§500å††ã®æœ¬ã‚’2å†Šã¨300å††ã®é›‘èªŒã‚’1å†Šè²·ã„ã¾ã—ãŸã€‚åˆè¨ˆé‡‘é¡ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
    "answer": "1300å††",
}

print("\nã€ãƒ†ã‚¹ãƒˆå•é¡Œã€‘")
print(f"å•é¡Œ: {test_problem['question']}")
print(f"æ­£è§£: {test_problem['answer']}")

# --- å®Ÿé¨“1: é€šå¸¸ã®Few-shot ---
print("\n" + "-" * 70)
print("å®Ÿé¨“1: é€šå¸¸ã®Few-shotï¼ˆæ€è€ƒéç¨‹ãªã—ï¼‰")
print("-" * 70)

standard_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œï¼ˆç­”ãˆã®ã¿ï¼‰
for i, example in enumerate(math_problems[:2], 1):
    standard_prompt += f"å•é¡Œ: {example['question']}\n"
    standard_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

# ãƒ†ã‚¹ãƒˆå•é¡Œ
standard_prompt += f"å•é¡Œ: {test_problem['question']}\nç­”ãˆ:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{standard_prompt}")

try:
    output_standard = generator(
        standard_prompt,
        max_new_tokens=30,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )

    result_standard = output_standard[0]["generated_text"][
        len(standard_prompt) :
    ].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸç­”ãˆ: {result_standard[:50]}")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_standard = "ã‚¨ãƒ©ãƒ¼"

# --- å®Ÿé¨“2: CoT Few-shot ---
print("\n" + "-" * 70)
print("å®Ÿé¨“2: CoT Few-shotï¼ˆæ€è€ƒéç¨‹ã‚ã‚Šï¼‰")
print("-" * 70)

cot_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’ã€è¨ˆç®—éç¨‹ã‚’ç¤ºã—ãªãŒã‚‰è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œï¼ˆæ€è€ƒéç¨‹ä»˜ãï¼‰
for i, example in enumerate(math_problems[:2], 1):
    cot_prompt += f"å•é¡Œ: {example['question']}\n"
    cot_prompt += f"è€ƒãˆæ–¹: {example['reasoning']}\n"
    cot_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

# ãƒ†ã‚¹ãƒˆå•é¡Œ
cot_prompt += f"å•é¡Œ: {test_problem['question']}\nè€ƒãˆæ–¹:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{cot_prompt}")

try:
    output_cot = generator(
        cot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )

    result_cot = output_cot[0]["generated_text"][len(cot_prompt) :].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹ã¨ç­”ãˆ: {result_cot[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_cot = "ã‚¨ãƒ©ãƒ¼"

# --- å®Ÿé¨“3: Zero-shot CoT ---
print("\n" + "-" * 70)
print("å®Ÿé¨“3: Zero-shot CoTï¼ˆã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ ï¼‰")
print("-" * 70)

zero_shot_cot_prompt = f"""ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚æ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€‚

å•é¡Œ: {test_problem["question"]}

è€ƒãˆæ–¹:"""

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{zero_shot_cot_prompt}")

try:
    output_zero_cot = generator(
        zero_shot_cot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )

    result_zero_cot = output_zero_cot[0]["generated_text"][
        len(zero_shot_cot_prompt) :
    ].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹: {result_zero_cot[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_zero_cot = "ã‚¨ãƒ©ãƒ¼"

# çµæœæ¯”è¼ƒ
print("\n" + "=" * 70)
print("ã€CoTåŠ¹æœã®æ¯”è¼ƒã€‘")
print("=" * 70)
print(f"æ­£è§£: {test_problem['answer']}")
print(f"\né€šå¸¸Few-shot:    {result_standard[:50]}")
print(f"CoT Few-shot:    {result_cot[:50]}")
print(f"Zero-shot CoT:   {result_zero_cot[:50]}")
print("\nğŸ’¡ CoTã§ã¯æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ¨è«–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")

# ============================================
# æ¼”ç¿’4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µ
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µ")
print("=" * 70)

print("""
åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®åŸå‰‡ã‚’å®Ÿè·µã—ã¾ã™ã€‚
åŒã˜ã‚¿ã‚¹ã‚¯ã§ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹ã§çµæœãŒå¤§ããå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
""")

# ã‚¿ã‚¹ã‚¯: ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆæ„Ÿæƒ…åˆ†æï¼‰
sentiment_samples = [
    {
        "text": "ã“ã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®æ–™ç†ã¯æœ€é«˜ã§ã—ãŸï¼ã¾ãŸæ¥ãŸã„ã§ã™ã€‚",
        "sentiment": "ãƒã‚¸ãƒ†ã‚£ãƒ–",
    },
    {"text": "ã‚µãƒ¼ãƒ“ã‚¹ãŒæ‚ªãã¦äºŒåº¦ã¨è¡ŒããŸããªã„ã§ã™ã€‚", "sentiment": "ãƒã‚¬ãƒ†ã‚£ãƒ–"},
    {"text": "æ–™ç†ã¯ç¾å‘³ã—ã„ã‘ã©ã€å€¤æ®µãŒé«˜ã™ãã¾ã™ã€‚", "sentiment": "ä¸­ç«‹"},
]

test_text_sentiment = "é›°å›²æ°—ã¯è‰¯ã‹ã£ãŸã§ã™ãŒã€æ–™ç†ãŒå†·ã‚ã¦ã„ã¦æ®‹å¿µã§ã—ãŸã€‚"

print("\nã€ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‘")
print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text_sentiment}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ›–æ˜§ãªæŒ‡ç¤º
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ›–æ˜§ãªæŒ‡ç¤º")
print("-" * 70)

vague_prompt = f"æ„Ÿæƒ…ã‚’æ•™ãˆã¦ã€‚\n\n{test_text_sentiment}\n\næ„Ÿæƒ…:"
print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{vague_prompt}")

try:
    output_vague = generator(
        vague_prompt,
        max_new_tokens=20,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )
    result_vague = output_vague[0]["generated_text"][len(vague_prompt) :].strip()
    print(f"\nçµæœ: {result_vague[:30]}")
except Exception as e:
    result_vague = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ˜ç¢ºãªæŒ‡ç¤º + å½¢å¼æŒ‡å®š
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ˜ç¢ºãªæŒ‡ç¤º + å‡ºåŠ›å½¢å¼ã®æŒ‡å®š")
print("-" * 70)

clear_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
æ„Ÿæƒ…ã®ã¿ã‚’ç­”ãˆã€èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {test_text_sentiment}

æ„Ÿæƒ…:"""

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{clear_prompt}")

try:
    output_clear = generator(
        clear_prompt,
        max_new_tokens=10,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )
    result_clear = output_clear[0]["generated_text"][len(clear_prompt) :].strip()
    print(f"\nçµæœ: {result_clear[:20]}")
except Exception as e:
    result_clear = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: å½¹å‰²è¨­å®š + Few-shot + å½¢å¼æŒ‡å®š
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³3: å½¹å‰²è¨­å®š + Few-shot + å½¢å¼æŒ‡å®š")
print("-" * 70)

best_prompt = """ã‚ãªãŸã¯æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ä¾‹:
ãƒ†ã‚­ã‚¹ãƒˆ: ã“ã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®æ–™ç†ã¯æœ€é«˜ã§ã—ãŸï¼ã¾ãŸæ¥ãŸã„ã§ã™ã€‚
æ„Ÿæƒ…: ãƒã‚¸ãƒ†ã‚£ãƒ–

ãƒ†ã‚­ã‚¹ãƒˆ: ã‚µãƒ¼ãƒ“ã‚¹ãŒæ‚ªãã¦äºŒåº¦ã¨è¡ŒããŸããªã„ã§ã™ã€‚
æ„Ÿæƒ…: ãƒã‚¬ãƒ†ã‚£ãƒ–

ãƒ†ã‚­ã‚¹ãƒˆ: æ–™ç†ã¯ç¾å‘³ã—ã„ã‘ã©ã€å€¤æ®µãŒé«˜ã™ãã¾ã™ã€‚
æ„Ÿæƒ…: ä¸­ç«‹

"""
best_prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text_sentiment}\næ„Ÿæƒ…:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¸€éƒ¨çœç•¥ï¼‰:\n{best_prompt[:200]}...")

try:
    output_best = generator(
        best_prompt,
        max_new_tokens=10,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id,
        truncation=True,
    )
    result_best = output_best[0]["generated_text"][len(best_prompt) :].strip()
    print(f"\nçµæœ: {result_best[:20]}")
except Exception as e:
    result_best = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# æ¯”è¼ƒè¡¨ç¤º
print("\n" + "=" * 70)
print("ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã«ã‚ˆã‚‹çµæœã®é•ã„ã€‘")
print("=" * 70)
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼ˆæ›–æ˜§ï¼‰:          {result_vague[:30]}")
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼ˆæ˜ç¢º+å½¢å¼ï¼‰:     {result_clear[:30]}")
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³3ï¼ˆå½¹å‰²+Few+å½¢å¼ï¼‰: {result_best[:30]}")
print("\nğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå…·ä½“çš„ã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ã»ã©ã€æœŸå¾…ã™ã‚‹å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™")

# ============================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
# ============================================

print("\n\n" + "=" * 70)
print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹")
print("=" * 70)

best_practices = """
ã€1. æ˜ç¢ºã§å…·ä½“çš„ãªæŒ‡ç¤ºã€‘
âœ“ ã‚¿ã‚¹ã‚¯ã‚’æ˜ç¢ºã«å®šç¾©ã™ã‚‹
âœ“ æ›–æ˜§ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹
âœ— ã€Œæ„Ÿæƒ…ã‚’æ•™ãˆã¦ã€â†’ âœ“ ã€Œæ„Ÿæƒ…ã‚’ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ä¸­ç«‹ã§åˆ†é¡ã€

ã€2. å‡ºåŠ›å½¢å¼ã®æŒ‡å®šã€‘
âœ“ æœŸå¾…ã™ã‚‹å‡ºåŠ›ã®å½¢å¼ã‚’æ˜ç¤ºã™ã‚‹
âœ“ ç®‡æ¡æ›¸ãã€JSONã€å˜èªã®ã¿ãªã©
âœ— å½¢å¼æŒ‡å®šãªã— â†’ âœ“ ã€Œæ„Ÿæƒ…ã®ã¿ã‚’1å˜èªã§ç­”ãˆã¦ãã ã•ã„ã€

ã€3. å½¹å‰²ã®è¨­å®šã€‘
âœ“ ã€Œã‚ãªãŸã¯ã€œã®å°‚é–€å®¶ã§ã™ã€ã¨å½¹å‰²ã‚’ä¸ãˆã‚‹
âœ“ ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸæ–‡è„ˆã‚’æä¾›
ä¾‹: ã€Œã‚ãªãŸã¯æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€

ã€4. ä¾‹é¡Œã®æä¾›ï¼ˆFew-shotï¼‰ã€‘
âœ“ å…¥åŠ›ã¨æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ã®ä¾‹ã‚’ç¤ºã™
âœ“ å¤šæ§˜ãªä¾‹ã‚’2-5å€‹ç¨‹åº¦
âœ“ ä¾‹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®

ã€5. åˆ¶ç´„æ¡ä»¶ã®æ˜ç¤ºã€‘
âœ“ æ–‡å­—æ•°ã€ä½¿ç”¨èªå½™ã€ãƒˆãƒ¼ãƒ³ãªã©ã‚’æŒ‡å®š
ä¾‹: ã€Œ100æ–‡å­—ä»¥å†…ã§ã€ã€Œå°‚é–€ç”¨èªã‚’é¿ã‘ã¦ã€

ã€6. æ®µéšçš„æ€è€ƒã®ä¿ƒé€²ï¼ˆCoTï¼‰ã€‘
âœ“ ã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ 
âœ“ è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§åŠ¹æœçš„
âœ“ ä¾‹é¡Œã«æ€è€ƒéç¨‹ã‚’å«ã‚ã‚‹

ã€7. å¦å®šå½¢ã‚ˆã‚Šè‚¯å®šå½¢ã€‘
âœ— ã€Œã€œã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€
âœ“ ã€Œã€œã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„ã€
"""

print(best_practices)

# ============================================
# ç™ºå±•èª²é¡Œ
# ============================================

print("\n\n" + "=" * 70)
print("ç™ºå±•èª²é¡Œï¼ˆä»»æ„ï¼‰")
print("=" * 70)

advanced_exercises = """
ä»¥ä¸‹ã®èª²é¡Œã«æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†:

ã€èª²é¡ŒAã€‘ç‹¬è‡ªã‚¿ã‚¹ã‚¯ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–
1. è‡ªåˆ†ã§èˆˆå‘³ã®ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é¸ã¶ï¼ˆç¿»è¨³ã€è¦ç´„ã€åˆ†é¡ãªã©ï¼‰
2. æ§˜ã€…ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
3. æœ€ã‚‚åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦‹ã¤ã‘ã‚‹
4. æ”¹å–„ã®ãƒã‚¤ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã‚‹

ã€èª²é¡ŒBã€‘Few-shotä¾‹é¡Œã®é¸æŠå®Ÿé¨“
1. åŒã˜ã‚¿ã‚¹ã‚¯ã§ç•°ãªã‚‹ä¾‹é¡Œã‚»ãƒƒãƒˆã‚’ç”¨æ„
2. ä¾‹é¡Œã®æ•°ï¼ˆ1, 3, 5å€‹ãªã©ï¼‰ã‚’å¤‰ãˆã¦æ¯”è¼ƒ
3. ä¾‹é¡Œã®è³ªï¼ˆå¤šæ§˜æ€§ã€ãƒãƒ©ãƒ³ã‚¹ï¼‰ãŒçµæœã«ä¸ãˆã‚‹å½±éŸ¿ã‚’åˆ†æ

ã€èª²é¡ŒCã€‘CoTã®åŠ¹æœæ¤œè¨¼
1. è¤‡æ•°ã®æ¨è«–ã‚¿ã‚¹ã‚¯ã‚’ç”¨æ„
2. é€šå¸¸ã®Few-shot vs CoT Few-shotã§æ¯”è¼ƒ
3. ã©ã®ã‚ˆã†ãªã‚¿ã‚¹ã‚¯ã§CoTãŒç‰¹ã«åŠ¹æœçš„ã‹è€ƒå¯Ÿ

ã€èª²é¡ŒDã€‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
1. æ±ç”¨çš„ã«ä½¿ãˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨­è¨ˆ
2. è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã§è©¦ã—ã¦æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼
3. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–

ã€èª²é¡ŒEã€‘å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
1. livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã®åˆ¥ã‚«ãƒ†ã‚´ãƒªã§åˆ†é¡ç²¾åº¦ã‚’æ¸¬å®š
2. ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã§ç²¾åº¦ã‚’æ¯”è¼ƒ
3. çµæœã‚’å®šé‡çš„ã«åˆ†æ
"""

print(advanced_exercises)

# ============================================
# å‚è€ƒãƒªã‚½ãƒ¼ã‚¹
# ============================================

print("\n\n" + "=" * 70)
print("å‚è€ƒãƒªã‚½ãƒ¼ã‚¹ - ã•ã‚‰ã«å­¦ã¶ãŸã‚ã«")
print("=" * 70)

resources = """
ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‘
â€¢ OpenAI Prompt Engineering Guide
  https://platform.openai.com/docs/guides/prompt-engineering
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å…¬å¼ã‚¬ã‚¤ãƒ‰

ã€æŠ€è¡“è§£èª¬ã€‘
â€¢ Prompt Engineering by Lilian Weng
  https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
  - åŒ…æ‹¬çš„ãªæŠ€è¡“è§£èª¬ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã€‘
â€¢ Prompt Engineering Guide (GitHub)
  https://github.com/dair-ai/Prompt-Engineering-Guide
  - è«–æ–‡ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€å®Ÿä¾‹ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
  - å¤šè¨€èªå¯¾å¿œï¼ˆæ—¥æœ¬èªã‚ã‚Šï¼‰

â€¢ Awesome ChatGPT Prompts
  https://github.com/f/awesome-chatgpt-prompts
  - æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹é›†
  - ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§å…±æœ‰ã•ã‚ŒãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
â€¢ Hugging Face Datasets
  https://huggingface.co/datasets
  - å…¬é–‹ã•ã‚Œã¦ã„ã‚‹å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
  - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚è±Šå¯Œ

ã€ãƒ¢ãƒ‡ãƒ«ã€‘
â€¢ Hugging Face Model Hub
  https://huggingface.co/models
  - APIã‚­ãƒ¼ä¸è¦ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ãŒå¤šæ•°
  - æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚‚å……å®Ÿ
"""

print(resources)

# ============================================
# ã¾ã¨ã‚
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’ã®ã¾ã¨ã‚")
print("=" * 70)

summary = """
ã€æœ¬æ—¥å­¦ã‚“ã ã“ã¨ã€‘

1ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤
   âœ“ Zero-shot vs Few-shotã®é•ã„ã¨åŠ¹æœ
   âœ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆãŒå‡ºåŠ›å“è³ªã«å¤§ããå½±éŸ¿
   âœ“ æ˜ç¢ºãªæŒ‡ç¤ºã¨å½¢å¼æŒ‡å®šã®é‡è¦æ€§

2ï¸âƒ£ In-Context Learning
   âœ“ ä¾‹é¡Œã‹ã‚‰å­¦ç¿’ã™ã‚‹èƒ½åŠ›
   âœ“ ä¾‹é¡Œã®æ•°ã¨è³ªãŒæ€§èƒ½ã«å½±éŸ¿
   âœ“ é©åˆ‡ãªä¾‹é¡Œé¸æŠã®é‡è¦æ€§

3ï¸âƒ£ Chain-of-Thought Prompting
   âœ“ æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§æ¨è«–èƒ½åŠ›å‘ä¸Š
   âœ“ è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã§ç‰¹ã«åŠ¹æœçš„
   âœ“ Zero-shot CoTã§ã‚‚åŠ¹æœã‚ã‚Š

4ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
   âœ“ å½¹å‰²è¨­å®šã€å½¢å¼æŒ‡å®šã€åˆ¶ç´„æ¡ä»¶ã®æ˜ç¤º
   âœ“ æ®µéšçš„æ€è€ƒã®ä¿ƒé€²
   âœ“ é©åˆ‡ãªä¾‹é¡Œã®æä¾›

ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
â€¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ŒæŒ‡ç¤ºæ›¸ã€- å…·ä½“çš„ã§æ˜ç¢ºã«
â€¢ ä¾‹é¡Œã¯ã€ŒãŠæ‰‹æœ¬ã€- å¤šæ§˜æ€§ã¨ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®
â€¢ CoTã¯ã€Œæ€è€ƒã®è¦‹ãˆã‚‹åŒ–ã€- æ¨è«–ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›
â€¢ å®Ÿé¨“ã¨æ”¹å–„ã®ã‚µã‚¤ã‚¯ãƒ«ãŒé‡è¦

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
â€¢ æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ã¦ã¿ã‚‹
â€¢ å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡å®Ÿé¨“ã‚’è¡Œã†
â€¢ æœ€æ–°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã‚’å­¦ã¶ï¼ˆå‚è€ƒãƒªã‚½ãƒ¼ã‚¹å‚ç…§ï¼‰
â€¢ ç‹¬è‡ªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹
"""

print(summary)

print("\n" + "=" * 70)
print("æ¼”ç¿’çµ‚äº† - ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")
print("=" * 70)
print("\nğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¯LLMã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹éµã§ã™")
print("   å®Ÿè·µã‚’é€šã˜ã¦ã€è‡ªåˆ†ãªã‚Šã®ãƒã‚¦ãƒã‚¦ã‚’è“„ç©ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼")
