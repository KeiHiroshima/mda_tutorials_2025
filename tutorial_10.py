"""
MDAå…¥é–€ 2025å¹´åº¦ ç¬¬10å›æ¼”ç¿’ï¼ˆä¿®æ­£ç‰ˆï¼‰
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°

æ¼”ç¿’å†…å®¹:
1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®å®Ÿè·µï¼ˆZero-shot / Few-shotï¼‰
2. In-Context Learningã®ä½“é¨“
3. Chain-of-Thought (CoT) Promptingã®å®Ÿé¨“

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆå„ªå…ˆé †ä½é †ï¼‰:
1. Qwen/Qwen2.5-0.5B-Instructï¼ˆæœ€å„ªå…ˆï¼‰
2. llm-jp/llm-jp-3-1.8b-instruct
3. rinna/japanese-gpt2-medium
4. cyberagent/open-calm-smallï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
- JGLUE/MARC-jaï¼ˆæ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
- tyqiangz/multilingual-sentimentsï¼ˆå¤šè¨€èªæ„Ÿæƒ…åˆ†æï¼‰
"""

# ============================================
# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ============================================

print("=" * 70)
print("ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
print("=" * 70)

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
#!pip install -q transformers torch datasets accelerate sentencepiece

import gc
import random
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

warnings.filterwarnings("ignore")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


# ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def generate_prediction(prompt, generator, tokenizer):
    try:
        output = generator(
            prompt,
            max_new_tokens=5, # ç­”ãˆã ã‘æ¬²ã—ã„ã®ã§çŸ­ãã¦è‰¯ã„
            num_return_sequences=1,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,
            truncation=True
        )
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»¥é™ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        generated = output[0]["generated_text"][len(prompt):].strip()
        # æœ€åˆã®æ”¹è¡Œã‚„ç©ºç™½ã¾ã§ã‚’å–å¾—
        return generated.split('\n')[0].strip()
    except Exception as e:
        return "Error"


def build_prompt(examples, target_text, randomize_labels=False):
    prompt = "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"

    # ãƒ©ãƒ™ãƒ«ã®å€™è£œ
    labels = ["ãƒã‚¸ãƒ†ã‚£ãƒ–", "ãƒã‚¬ãƒ†ã‚£ãƒ–", "ä¸­ç«‹"]

    for i, ex in enumerate(examples):
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã®å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰
        if isinstance(ex, dict):
             txt = ex.get("text", ex.get("review", ex.get("sentence", "")))
             lbl = ex.get("label", ex.get("sentiment", ""))
        else:
             txt = str(list(ex.values())[0])
             lbl = "unknown"

        txt = str(txt)[:80] # é•·ã™ãã‚‹ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åœ§è¿«ã™ã‚‹ã®ã§ã‚«ãƒƒãƒˆ

        if randomize_labels:
            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
            lbl = random.choice(labels)

        prompt += f"ä¾‹{i+1}:\nãƒ†ã‚­ã‚¹ãƒˆ: {txt}...\næ„Ÿæƒ…: {lbl}\n\n"

    prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {target_text}\næ„Ÿæƒ…:"
    return prompt


def main():
    set_seed(42)

    print("\nâœ“ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    print("=" * 70)
    print("MDAå…¥é–€ ç¬¬10å›æ¼”ç¿’: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°")
    print("=" * 70)

    # ============================================
    # å…¬é–‹LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’é †æ¬¡è©¦è¡Œï¼‰
    # ============================================

    print("\n" + "=" * 70)
    print("å…¬é–‹LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
    print("=" * 70)

    # è©¦è¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    model_candidates = [
        {
            "name": "Qwen/Qwen2.5-0.5B-Instruct",
            "params": "500M",
            "description": "Alibabaé–‹ç™ºã€å¤šè¨€èªå¯¾å¿œã€Instructionãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿",
        },
        {
            "name": "llm-jp/llm-jp-3-1.8b-instruct",
            "params": "1.8B",
            "description": "å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€é–‹ç™ºã€æ—¥æœ¬èªç‰¹åŒ–",
        },
        {
            "name": "rinna/japanese-gpt2-medium",
            "params": "330M",
            "description": "æ—¥æœ¬èªGPT-2ã€å®Ÿç¸¾è±Šå¯Œ",
        },
        {
            "name": "cyberagent/open-calm-small",
            "params": "160M",
            "description": "ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™ºã€è¶…è»½é‡",
        },
    ]

    # ãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(
            f"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )

    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ
    model = None
    tokenizer = None
    model_name = None
    loaded_model_info = None

    print("\n" + "-" * 70)
    print("ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰è©¦è¡Œï¼ˆå„ªå…ˆé †ä½é †ï¼‰")
    print("-" * 70)

    for i, candidate in enumerate(model_candidates, 1):
        model_name = candidate["name"]
        print(f"\nã€è©¦è¡Œ {i}/4ã€‘{model_name}")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {candidate['params']}")
        print(f"  ç‰¹å¾´: {candidate['description']}")
        print("  èª­ã¿è¾¼ã¿ä¸­...", end="")

        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            tokenizer = AutoTokenizer.from_pretrained(model_name)

            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto",
                low_cpu_mem_usage=True,
            )

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª
            num_params = sum(p.numel() for p in model.parameters()) / 1e6

            print(" âœ“ æˆåŠŸï¼")
            print(f"  å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:.0f}M")

            loaded_model_info = candidate
            break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

        except Exception as e:
            print(" âœ— å¤±æ•—")
            print(f"  ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}...")

            # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            if model is not None:
                del model
            if tokenizer is not None:
                del tokenizer
            gc.collect()
            if device == "cuda":
                torch.cuda.empty_cache()

            model = None
            tokenizer = None

            if i < len(model_candidates):
                print("  â†’ æ¬¡ã®å€™è£œãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™...")
            else:
                print("  â†’ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")

    # ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
    if model is None or tokenizer is None:
        raise RuntimeError(
            "âŒ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Google Colabã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )

    print("\n" + "=" * 70)
    print(f"âœ“ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {loaded_model_info['params']}")
    print(f"  ç‰¹å¾´: {loaded_model_info['description']}")
    print("=" * 70)

    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
    )  # device=0 if device == "cuda" else -1,

    print("\nâœ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™å®Œäº†")

    # ============================================
    # å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    # ============================================

    print("\n" + "=" * 70)
    print("å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿")
    print("=" * 70)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå€™è£œï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    dataset_candidates = [
        {
            "name": "tyqiangz/multilingual-sentiments",
            "config": "japanese",
            "description": "å¤šè¨€èªæ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ—¥æœ¬èªï¼‰",
        },
        {
            "name": "shunk031/JGLUE",
            "config": "MARC-ja",
            "description": "æ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆAmazonï¼‰",
        },
    ]

    dataset = None
    dataset_info = None

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    if dataset is None:
        print("ä»£æ›¿ã¨ã—ã¦æ‰‹å‹•ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™\n")

        # ä»£æ›¿ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬èªæ„Ÿæƒ…åˆ†æï¼‰
        dataset = [
            {"text": "ã“ã®å•†å“ã¯æœ€é«˜ã§ã™ï¼è²·ã£ã¦è‰¯ã‹ã£ãŸã§ã™ã€‚", "label": "positive"},
            {"text": "æœŸå¾…å¤–ã‚Œã§ã—ãŸã€‚å“è³ªãŒæ‚ªã™ãã¾ã™ã€‚", "label": "negative"},
            {"text": "å€¤æ®µã®å‰²ã«ã¯æ™®é€šã§ã™ã€‚å¯ã‚‚ãªãä¸å¯ã‚‚ãªãã€‚", "label": "neutral"},
            {
                "text": "ç´ æ™´ã‚‰ã—ã„è£½å“ã§ã™ã€‚å‹äººã«ã‚‚å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã€‚",
                "label": "positive",
            },
            {"text": "ã‚µãƒ¼ãƒ“ã‚¹ãŒæœ€æ‚ªã§ã—ãŸã€‚äºŒåº¦ã¨åˆ©ç”¨ã—ã¾ã›ã‚“ã€‚", "label": "negative"},
            {"text": "æ©Ÿèƒ½ã¯è‰¯ã„ã§ã™ãŒã€ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã‚¤ãƒã‚¤ãƒã§ã™ã€‚", "label": "neutral"},
            {"text": "æœŸå¾…ä»¥ä¸Šã®æ€§èƒ½ã§ã—ãŸã€‚å¤§æº€è¶³ã§ã™ï¼", "label": "positive"},
            {"text": "èª¬æ˜ã¨å®Ÿç‰©ãŒå…¨ç„¶é•ã†ã€‚è©æ¬ºãƒ¬ãƒ™ãƒ«ã§ã™ã€‚", "label": "negative"},
            {"text": "ã“ã®ä¾¡æ ¼ãªã‚‰å¦¥å½“ã ã¨æ€ã„ã¾ã™ã€‚", "label": "neutral"},
            {"text": "æ„Ÿå‹•ã—ã¾ã—ãŸã€‚æœ¬å½“ã«è²·ã£ã¦è‰¯ã‹ã£ãŸã§ã™ã€‚", "label": "positive"},
        ]

        dataset_info = {
            "name": "æ‰‹å‹•ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿",
            "description": "æ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼æ„Ÿæƒ…åˆ†æ",
        }

        print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†ï¼ˆ{len(dataset)}ä»¶ï¼‰")

    else:
        print(f"\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {dataset_info['name']}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print("\nã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã€‘")
        sample = dataset[0] if isinstance(dataset, list) else dataset[0]
        for key, value in sample.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {key}: {value[:100]}...")
            else:
                print(f"  {key}: {value}")

    # ============================================
    # æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ
    # ============================================

    print("\n\n" + "=" * 70)
    print("æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ")
    print("=" * 70)

    """
    ã“ã®æ¼”ç¿’ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆãŒå‡ºåŠ›ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’å­¦ã³ã¾ã™ã€‚
    - Zero-shot promptingï¼ˆä¾‹é¡Œãªã—ï¼‰
    - Few-shot promptingï¼ˆä¾‹é¡Œã‚ã‚Šï¼‰
    ã®é•ã„ã‚’å®Ÿéš›ã«ç¢ºèªã—ã¾ã™ã€‚
    """

    # --- ã‚¿ã‚¹ã‚¯1: ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æ ---
    print("\n" + "-" * 70)
    print("ã‚¿ã‚¹ã‚¯1: ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æ")
    print("-" * 70)

    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
    if isinstance(dataset, list):
        test_text = dataset[0]["text"]
        test_label = dataset[0]["label"]
    else:
        test_idx = 50
        sample = dataset[test_idx]
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        if "text" in sample:
            test_text = sample["text"]
        elif "sentence" in sample:
            test_text = sample["sentence"]
        elif "review" in sample:
            test_text = sample["review"]
        else:
            test_text = str(list(sample.values())[0])

        if "label" in sample:
            test_label = sample["label"]
        elif "sentiment" in sample:
            test_label = sample["sentiment"]
        else:
            test_label = "unknown"

    print("\nã€ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‘")
    print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
    print(f"æ­£è§£ãƒ©ãƒ™ãƒ«: {test_label}")

    # Zero-shot prompting
    print("\nã€å®Ÿé¨“1ã€‘Zero-shot Prompting")
    print("-" * 50)

    zero_shot_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}

æ„Ÿæƒ…:"""

    print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
    print(zero_shot_prompt[:150] + "...")

    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    try:
        output_zero = generator(
            zero_shot_prompt,
            max_new_tokens=20,
            num_return_sequences=1,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
            if tokenizer.eos_token_id
            else tokenizer.pad_token_id,
            truncation=True,
        )

        generated_text = output_zero[0]["generated_text"]
        summary_zero = generated_text[len(zero_shot_prompt) :].strip()

        print("\nç”Ÿæˆã•ã‚ŒãŸåˆ†é¡çµæœ:")
        print(f"{summary_zero[:50]}")

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        summary_zero = "ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"

    # çµæœè¡¨ç¤º
    print("\n" + "=" * 70)
    print("ã€Zero-shot çµæœã€‘")
    print("=" * 70)
    print(f"æ­£è§£:       {test_label}")
    print(f"äºˆæ¸¬:       {summary_zero[:30]}")

    # ============================================
    # æ¼”ç¿’2: In-Context Learning (ICL) ã®å®Ÿé¨“
    # ============================================

    print("\n\n" + "=" * 70)
    print("æ¼”ç¿’2: In-Context Learning (ICL) ã®å®Ÿé¨“")
    print("=" * 70)

    print("""
Few-shot ICLã«ãŠã‘ã‚‹ä»¥ä¸‹ã®è¦ç´ ã®å½±éŸ¿ã‚’å®Ÿé¨“ãƒ»æ¯”è¼ƒã—ã¾ã™ï¼š
1. æ¨™æº–çš„ãªFew-shotï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã€å›ºå®šé †åºï¼‰
2. ãƒ©ãƒ™ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼ˆäº‹ä¾‹ã®ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å…¥ã‚Œæ›¿ãˆï¼‰
3. äº‹ä¾‹é †åºã®å¤‰æ›´ï¼ˆäº‹ä¾‹ã®æç¤ºé †ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼‰
""")

    # è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    n_shots = 3
    if isinstance(dataset, list):
        # æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
        icl_examples = dataset[:n_shots]      # ä¾‹ç¤ºç”¨
        icl_test_data = dataset[n_shots:n_shots+5]    # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆæœ€å¤§5ä»¶ï¼‰
    else:
        # HFãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆ
        icl_examples = [dataset[i] for i in range(n_shots)]
        icl_test_data = [dataset[i] for i in range(n_shots, n_shots+5)]

    # --- å®Ÿé¨“è¨­å®š ---
    conditions = [
        {"name": "æ¨™æº–Few-shot", "random_label": False, "shuffle_order": False},
        {"name": "ãƒ©ãƒ™ãƒ«ãƒ©ãƒ³ãƒ€ãƒ ", "random_label": True, "shuffle_order": False},
        {"name": "é †åºå¤‰æ›´",     "random_label": False, "shuffle_order": True},
    ]

    # å®Ÿé¨“ãƒ«ãƒ¼ãƒ—
    results = {c["name"]: [] for c in conditions}

    print(f"\nè©•ä¾¡ãƒ‡ãƒ¼ã‚¿æ•°: {len(icl_test_data)}ä»¶ (å„æ¡ä»¶ã§æ¨è«–ã‚’å®Ÿè¡Œ)")
    print("-" * 60)

    for idx, sample in enumerate(icl_test_data):
        # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
        if isinstance(sample, dict):
            test_text = sample.get("text", sample.get("review", sample.get("sentence", "")))
            true_label = sample.get("label", sample.get("sentiment", ""))
        else:
            test_text = str(list(sample.values())[0])
            true_label = "unknown"

        print(f"\nãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {idx+1}: {test_text[:30]}... (æ­£è§£: {true_label})")

        for cond in conditions:
            current_examples = list(icl_examples) # ã‚³ãƒ”ãƒ¼

            # é †åºå¤‰æ›´
            if cond["shuffle_order"]:
                random.shuffle(current_examples)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = build_prompt(
                current_examples,
                test_text,
                randomize_labels=cond["random_label"]
            )

            # æ¨è«–
            pred = generate_prediction(prompt, generator, tokenizer)

            # çµæœä¿å­˜
            # ç°¡æ˜“æ­£è§£åˆ¤å®šï¼ˆæ–‡å­—åˆ—ã¨ã—ã¦æ­£è§£ãƒ©ãƒ™ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰
            is_correct = False
            if isinstance(true_label, str) and true_label in pred:
                is_correct = True

            results[cond["name"]].append(is_correct)

            mark = "â—‹" if is_correct else "âœ—"
            print(f"  [{cond['name']}] äºˆæ¸¬: {pred} ({mark})")

    # é›†è¨ˆçµæœ
    print("\n" + "=" * 60)
    print("ã€å®Ÿé¨“çµæœé›†è¨ˆï¼ˆæ­£è§£ç‡ï¼‰ã€‘")
    print("=" * 60)
    for name, res in results.items():
        if len(res) > 0:
            acc = sum(res) / len(res) * 100
            print(f"{name}: {acc:.1f}% ({sum(res)}/{len(res)})")
        else:
             print(f"{name}: ãƒ‡ãƒ¼ã‚¿ãªã—")

    print("\nğŸ’¡ è€ƒå¯Ÿ:")
    print("- ICLã§ã¯ã€Œãƒ©ãƒ™ãƒ«ã®æ­£ç¢ºã•ã€ã‚ˆã‚Šã‚‚ã€Œã‚¿ã‚¹ã‚¯ã®å½¢å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ã€ã‚„ã€Œå…¥åŠ›åˆ†å¸ƒã€ãŒé‡è¦ã§ã‚ã‚‹ã¨ã„ã†ç ”ç©¶çµæœãŒã‚ã‚Šã¾ã™ï¼ˆMin et al., 2022ãªã©ï¼‰ã€‚")
    print("- ãã®ãŸã‚ã€ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ã‚‚æ€§èƒ½ãŒå¤§ããè½ã¡ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
    print("- ä¸€æ–¹ã§ã€æç¤ºé †åºï¼ˆRecency Biasãªã©ï¼‰ã¯æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

    # ============================================
    # æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting
    # ============================================

    print("\n\n" + "=" * 70)
    print("æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting")
    print("=" * 70)

    print("""
æ€è€ƒéç¨‹ã‚’ä¾‹é¡Œã«å«ã‚ã‚‹ã“ã¨ã§ã€æ¨è«–èƒ½åŠ›ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
ã‚¿ã‚¹ã‚¯: ç°¡å˜ãªç®—æ•°ã®æ–‡ç« é¡Œ
""")

    # --- ç®—æ•°æ–‡ç« é¡Œã®ã‚µãƒ³ãƒ—ãƒ« ---
    math_problems = [
        {
            "question": "å¤ªéƒã¯5å€‹ã®ã‚Šã‚“ã”ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚3å€‹ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "answer": "8å€‹",
            "reasoning": "æœ€åˆã«5å€‹æŒã£ã¦ã„ã¦ã€3å€‹ã‚‚ã‚‰ã£ãŸã®ã§ã€5 + 3 = 8å€‹ã§ã™ã€‚",
        },
        {
            "question": "èŠ±å­ã¯10å€‹ã®ã¿ã‹ã‚“ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚4å€‹é£Ÿã¹ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "answer": "6å€‹",
            "reasoning": "æœ€åˆã«10å€‹æŒã£ã¦ã„ã¦ã€4å€‹é£Ÿã¹ãŸã®ã§ã€10 - 4 = 6å€‹ã§ã™ã€‚",
        },
        {
            "question": "ã‚¯ãƒ©ã‚¹ã«30äººã®ç”Ÿå¾’ãŒã„ã¾ã™ã€‚ãã®ã†ã¡12äººãŒç”·å­ã§ã™ã€‚å¥³å­ã¯ä½•äººã§ã™ã‹ï¼Ÿ",
            "answer": "18äºº",
            "reasoning": "å…¨ä½“ãŒ30äººã§ã€ç”·å­ãŒ12äººãªã®ã§ã€å¥³å­ã¯ 30 - 12 = 18äººã§ã™ã€‚",
        },
    ]

    # ãƒ†ã‚¹ãƒˆå•é¡Œ
    test_problem = {
        "question": "æœ¬å±‹ã§500å††ã®æœ¬ã‚’2å†Šã¨300å††ã®é›‘èªŒã‚’1å†Šè²·ã„ã¾ã—ãŸã€‚åˆè¨ˆé‡‘é¡ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
        "answer": "1300å††",
    }

    print("\nã€ãƒ†ã‚¹ãƒˆå•é¡Œã€‘")
    print(f"å•é¡Œ: {test_problem['question']}")
    print(f"æ­£è§£: {test_problem['answer']}")

    # --- å®Ÿé¨“1: é€šå¸¸ã®Few-shot ---
    print("\n" + "-" * 70)
    print("å®Ÿé¨“1: é€šå¸¸ã®Few-shotï¼ˆæ€è€ƒéç¨‹ãªã—ï¼‰")
    print("-" * 70)

    standard_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

    # ä¾‹é¡Œï¼ˆç­”ãˆã®ã¿ï¼‰
    for i, example in enumerate(math_problems[:2], 1):
        standard_prompt += f"å•é¡Œ: {example['question']}\n"
        standard_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

    # ãƒ†ã‚¹ãƒˆå•é¡Œ
    standard_prompt += f"å•é¡Œ: {test_problem['question']}\nç­”ãˆ:"

    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{standard_prompt}")

    try:
        output_standard = generator(
            standard_prompt,
            max_new_tokens=30,
            num_return_sequences=1,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
            if tokenizer.eos_token_id
            else tokenizer.pad_token_id,
            truncation=True,
        )

        result_standard = output_standard[0]["generated_text"][
            len(standard_prompt) :
        ].strip()
        print(f"\nç”Ÿæˆã•ã‚ŒãŸç­”ãˆ: {result_standard[:50]}")

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        result_standard = "ã‚¨ãƒ©ãƒ¼"

    # --- å®Ÿé¨“2: CoT Few-shot ---
    print("\n" + "-" * 70)
    print("å®Ÿé¨“2: CoT Few-shotï¼ˆæ€è€ƒéç¨‹ã‚ã‚Šï¼‰")
    print("-" * 70)

    cot_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’ã€è¨ˆç®—éç¨‹ã‚’ç¤ºã—ãªãŒã‚‰è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

    # ä¾‹é¡Œï¼ˆæ€è€ƒéç¨‹ä»˜ãï¼‰
    for i, example in enumerate(math_problems[:2], 1):
        cot_prompt += f"å•é¡Œ: {example['question']}\n"
        cot_prompt += f"è€ƒãˆæ–¹: {example['reasoning']}\n"
        cot_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

    # ãƒ†ã‚¹ãƒˆå•é¡Œ
    cot_prompt += f"å•é¡Œ: {test_problem['question']}\nè€ƒãˆæ–¹:"

    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{cot_prompt}")

    try:
        output_cot = generator(
            cot_prompt,
            max_new_tokens=50,
            num_return_sequences=1,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
            if tokenizer.eos_token_id
            else tokenizer.pad_token_id,
            truncation=True,
        )

        result_cot = output_cot[0]["generated_text"][len(cot_prompt) :].strip()
        print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹ã¨ç­”ãˆ: {result_cot[:100]}...")

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        result_cot = "ã‚¨ãƒ©ãƒ¼"

    # --- å®Ÿé¨“3: Zero-shot CoT ---
    print("\n" + "-" * 70)
    print("å®Ÿé¨“3: Zero-shot CoTï¼ˆã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ ï¼‰")
    print("-" * 70)

    zero_shot_cot_prompt = f"""ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚æ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€‚

å•é¡Œ: {test_problem["question"]}

è€ƒãˆæ–¹:"""

    print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{zero_shot_cot_prompt}")

    try:
        output_zero_cot = generator(
            zero_shot_cot_prompt,
            max_new_tokens=50,
            num_return_sequences=1,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
            if tokenizer.eos_token_id
            else tokenizer.pad_token_id,
            truncation=True,
        )

        result_zero_cot = output_zero_cot[0]["generated_text"][
            len(zero_shot_cot_prompt) :
        ].strip()
        print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹: {result_zero_cot[:100]}...")

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        result_zero_cot = "ã‚¨ãƒ©ãƒ¼"

    # çµæœæ¯”è¼ƒ
    print("\n" + "=" * 70)
    print("ã€CoTåŠ¹æœã®æ¯”è¼ƒã€‘")
    print("=" * 70)
    print(f"æ­£è§£: {test_problem['answer']}")
    print(f"\né€šå¸¸Few-shot:    {result_standard[:50]}")
    print(f"CoT Few-shot:    {result_cot[:50]}")
    print(f"Zero-shot CoT:   {result_zero_cot[:50]}")
    print("\nğŸ’¡ CoTã§ã¯æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ¨è«–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")



    # ============================================
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
    # ============================================

    print("\n\n" + "=" * 70)
    print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹")
    print("=" * 70)

    best_practices = """
ã€1. æ˜ç¢ºã§å…·ä½“çš„ãªæŒ‡ç¤ºã€‘
âœ“ ã‚¿ã‚¹ã‚¯ã‚’æ˜ç¢ºã«å®šç¾©ã™ã‚‹
âœ“ æ›–æ˜§ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹
âœ— ã€Œæ„Ÿæƒ…ã‚’æ•™ãˆã¦ã€â†’ âœ“ ã€Œæ„Ÿæƒ…ã‚’ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ä¸­ç«‹ã§åˆ†é¡ã€

ã€2. å‡ºåŠ›å½¢å¼ã®æŒ‡å®šã€‘
âœ“ æœŸå¾…ã™ã‚‹å‡ºåŠ›ã®å½¢å¼ã‚’æ˜ç¤ºã™ã‚‹
âœ“ ç®‡æ¡æ›¸ãã€JSONã€å˜èªã®ã¿ãªã©
âœ— å½¢å¼æŒ‡å®šãªã— â†’ âœ“ ã€Œæ„Ÿæƒ…ã®ã¿ã‚’1å˜èªã§ç­”ãˆã¦ãã ã•ã„ã€

ã€3. å½¹å‰²ã®è¨­å®šã€‘
âœ“ ã€Œã‚ãªãŸã¯ã€œã®å°‚é–€å®¶ã§ã™ã€ã¨å½¹å‰²ã‚’ä¸ãˆã‚‹
âœ“ ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸæ–‡è„ˆã‚’æä¾›
ä¾‹: ã€Œã‚ãªãŸã¯æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€

ã€4. ä¾‹é¡Œã®æä¾›ï¼ˆFew-shotï¼‰ã€‘
âœ“ å…¥åŠ›ã¨æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ã®ä¾‹ã‚’ç¤ºã™
âœ“ å¤šæ§˜ãªä¾‹ã‚’2-5å€‹ç¨‹åº¦
âœ“ ä¾‹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®

ã€5. åˆ¶ç´„æ¡ä»¶ã®æ˜ç¤ºã€‘
âœ“ æ–‡å­—æ•°ã€ä½¿ç”¨èªå½™ã€ãƒˆãƒ¼ãƒ³ãªã©ã‚’æŒ‡å®š
ä¾‹: ã€Œ100æ–‡å­—ä»¥å†…ã§ã€ã€Œå°‚é–€ç”¨èªã‚’é¿ã‘ã¦ã€

ã€6. æ®µéšçš„æ€è€ƒã®ä¿ƒé€²ï¼ˆCoTï¼‰ã€‘
âœ“ ã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ 
âœ“ è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§åŠ¹æœçš„
âœ“ ä¾‹é¡Œã«æ€è€ƒéç¨‹ã‚’å«ã‚ã‚‹

ã€7. å¦å®šå½¢ã‚ˆã‚Šè‚¯å®šå½¢ã€‘
âœ— ã€Œã€œã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€
âœ“ ã€Œã€œã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„ã€
"""

    print(best_practices)


    # ============================================
    # å‚è€ƒãƒªã‚½ãƒ¼ã‚¹
    # ============================================

    print("\n\n" + "=" * 70)
    print("å‚è€ƒãƒªã‚½ãƒ¼ã‚¹ - ã•ã‚‰ã«å­¦ã¶ãŸã‚ã«")
    print("=" * 70)

    resources = """
ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‘
â€¢ OpenAI Prompt Engineering Guide
  https://platform.openai.com/docs/guides/prompt-engineering
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å…¬å¼ã‚¬ã‚¤ãƒ‰

ã€æŠ€è¡“è§£èª¬ã€‘
â€¢ Prompt Engineering by Lilian Weng
  https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
  - åŒ…æ‹¬çš„ãªæŠ€è¡“è§£èª¬ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã€‘
â€¢ Prompt Engineering Guide (GitHub)
  https://github.com/dair-ai/Prompt-Engineering-Guide
  - è«–æ–‡ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€å®Ÿä¾‹ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
  - å¤šè¨€èªå¯¾å¿œï¼ˆæ—¥æœ¬èªã‚ã‚Šï¼‰

â€¢ Awesome ChatGPT Prompts
  https://github.com/f/awesome-chatgpt-prompts
  - æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹é›†
  - ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§å…±æœ‰ã•ã‚ŒãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
â€¢ Hugging Face Datasets
  https://huggingface.co/datasets
  - å…¬é–‹ã•ã‚Œã¦ã„ã‚‹å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
  - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚è±Šå¯Œ

ã€ãƒ¢ãƒ‡ãƒ«ã€‘
â€¢ Hugging Face Model Hub
  https://huggingface.co/models
  - APIã‚­ãƒ¼ä¸è¦ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ãŒå¤šæ•°
  - æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚‚å……å®Ÿ
"""

    print(resources)

    # ============================================
    # ã¾ã¨ã‚
    # ============================================

    print("\n\n" + "=" * 70)
    print("æ¼”ç¿’ã®ã¾ã¨ã‚")
    print("=" * 70)

    summary = f"""
ã€æœ¬æ—¥å­¦ã‚“ã ã“ã¨ã€‘

1ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤
   âœ“ Zero-shot ã®å‹•ä½œç¢ºèª
   âœ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®é‡è¦æ€§

2ï¸âƒ£ In-Context Learning (ICL)ã®æ€§è³ª
   âœ“ Few-shotã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚¹ã‚¯å½¢å¼ã‚’å­¦ç¿’
   âœ“ ãƒ©ãƒ™ãƒ«ã®æ­£ç¢ºã•ã‚ˆã‚Šã‚‚ã€Œå½¢å¼ã€ã‚„ã€Œåˆ†å¸ƒã€ãŒé‡è¦ï¼ˆLabel Chaosï¼‰
   âœ“ äº‹ä¾‹ã®æç¤ºé †åºãŒçµæœã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§

3ï¸âƒ£ Chain-of-Thought Prompting
   âœ“ æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§æ¨è«–èƒ½åŠ›å‘ä¸Š
   âœ“ è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã§ç‰¹ã«åŠ¹æœçš„

ã€ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‘
ãƒ¢ãƒ‡ãƒ«: {model_name}
ç‰¹å¾´: {loaded_model_info["description"]}

ã€ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_info["name"]}
èª¬æ˜: {dataset_info["description"]}

ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
â€¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ŒæŒ‡ç¤ºæ›¸ã€- å…·ä½“çš„ã§æ˜ç¢ºã«
â€¢ ICLã¯ä¸æ€è­°ãªæ€§è³ªã‚’æŒã¤ï¼ˆãƒ©ãƒ™ãƒ«ãŒé–“é•ã£ã¦ã„ã¦ã‚‚å‹•ãã“ã¨ãŒã‚ã‚‹ï¼‰
â€¢ CoTã¯ã€Œæ€è€ƒã®è¦‹ãˆã‚‹åŒ–ã€- æ¨è«–ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
â€¢ æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ã¦ã¿ã‚‹
â€¢ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹æŒ™å‹•ã®é•ã„ã‚’ç¢ºã‹ã‚ã‚‹
â€¢ æœ€æ–°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã‚’å­¦ã¶
"""

    print(summary)

if __name__ == "__main__":
    main()
