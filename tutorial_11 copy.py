"""
AIãƒ¢ãƒ‡ãƒ«ã‚’æ”¯ãˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ - æ¼”ç¿’è³‡æ–™
MDAå…¥é–€ ç¬¬12å›

æœ¬æ¼”ç¿’ã§ã¯ä»¥ä¸‹ã®å†…å®¹ã‚’æ‰±ã„ã¾ã™:
1. ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
2. CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
4. ä¸¦åˆ—è¨ˆç®—ã®åŠ¹æœå®Ÿæ„Ÿ

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
- numpy
- cupy (GPUãŒã‚ã‚‹å ´åˆ)
- matplotlib
- torch
- psutil
"""

import subprocess
import time
import warnings

import cupy as cp
import matplotlib.pyplot as plt
import numpy as np
import psutil
import torch
import torch.nn as nn

warnings.filterwarnings("ignore")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "DejaVu Sans"]

GPU_AVAILABLE = True

print("\nãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
print(f"PyTorch version: {torch.__version__}")
print(f"NumPy version: {np.__version__}")

# =====================================
# æ¼”ç¿’1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹è¨ˆç®—ç’°å¢ƒã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚
- GPUæƒ…å ±
- CPUæƒ…å ±
- ãƒ¡ãƒ¢ãƒªæƒ…å ±
- PyTorchã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
"""
)

# --- GPUæƒ…å ±ã®ç¢ºèª ---
print("\nã€GPUæƒ…å ±ã€‘")
print("-" * 70)

if torch.cuda.is_available():
    print("âœ“ CUDA is available")
    print(f"âœ“ CUDA version: {torch.version.cuda}")
    print(f"âœ“ Number of GPUs: {torch.cuda.device_count()}")

    for i in range(torch.cuda.device_count()):
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        gpu_memory = torch.cuda.get_device_properties(i).total_memory
        print(f"  Total Memory: {gpu_memory / 1e9:.2f} GB")

    # nvidia-smiã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
    print("\nã€nvidia-smi å‡ºåŠ›ã€‘")
    try:
        result = subprocess.run(
            ["nvidia-smi"], capture_output=True, text=True, check=True
        )
        print(result.stdout)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("nvidia-smi ã‚³ãƒãƒ³ãƒ‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

    device = torch.device("cuda")
    print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device} (GPU)")
else:
    print("âœ— CUDA is not available")
    print("CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
    device = torch.device("cpu")
    print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device} (CPU)")

# --- CPUæƒ…å ±ã®ç¢ºèª ---
print("\nã€CPUæƒ…å ±ã€‘")
print("-" * 70)

# lscpuã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆLinux/Macï¼‰
print("ã€CPUè©³ç´°æƒ…å ±ã€‘")
try:
    result = subprocess.run(
        ["lscpu"], capture_output=True, text=True, check=True, timeout=5
    )
    # é‡è¦ãªæƒ…å ±ã®ã¿æŠ½å‡º
    for line in result.stdout.split("\n"):
        if any(
            keyword in line
            for keyword in [
                "Model name",
                "CPU(s)",
                "Thread(s)",
                "Core(s)",
                "Socket(s)",
                "CPU MHz",
            ]
        ):
            print(line)
except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
    # Windowsã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰å¤±æ•—æ™‚ã¯pythonã§å–å¾—
    print(f"CPU ã‚³ã‚¢æ•°: {psutil.cpu_count(logical=False)} (ç‰©ç†)")
    print(f"CPU ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {psutil.cpu_count(logical=True)} (è«–ç†)")
    print(f"CPU ä½¿ç”¨ç‡: {psutil.cpu_percent(interval=1)}%")

# --- ãƒ¡ãƒ¢ãƒªæƒ…å ±ã®ç¢ºèª ---
print("\nã€ãƒ¡ãƒ¢ãƒªæƒ…å ±ã€‘")
print("-" * 70)

ram = psutil.virtual_memory()
print(f"Total RAM: {ram.total / 1e9:.2f} GB")
print(f"Available RAM: {ram.available / 1e9:.2f} GB")
print(f"Used RAM: {ram.used / 1e9:.2f} GB")
print(f"RAM Usage: {ram.percent}%")

# GPU Memory
if torch.cuda.is_available():
    print("\nGPU Memory (Device 0):")
    print(f"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    print(f"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")


# =====================================
# æ¼”ç¿’2: CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’2: CPU vs GPU é€Ÿåº¦æ¯”è¼ƒå®Ÿé¨“")
print("=" * 70)

if not GPU_AVAILABLE:
    print("\nâš ï¸ CuPyãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã“ã®æ¼”ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
    print("Google Colabãªã©ã®ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
else:
    print(
        """
ã“ã®æ¼”ç¿’ã§ã¯ã€CPUã¨GPUã§åŒã˜è¡Œåˆ—ç©ã®è¨ˆç®—ã‚’è¡Œã„ã€é€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
- NumPy (CPU) ã¨ CuPy (GPU) ã‚’ä½¿ç”¨
- è¡Œåˆ—ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦å®Ÿé¨“
- è¨ˆç®—æ™‚é–“ã¨é«˜é€ŸåŒ–ç‡ã‚’æ¸¬å®š
"""
    )

    # è¡Œåˆ—ã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ
    sizes = [100, 500, 1000, 2000, 5000]
    cpu_times = []
    gpu_times = []
    speedups = []

    print("\nè¡Œåˆ—ç©ã®è¨ˆç®—é€Ÿåº¦ã‚’æ¸¬å®šä¸­...")
    print("-" * 70)

    for n in sizes:
        print(f"\nè¡Œåˆ—ã‚µã‚¤ã‚º: {n} Ã— {n}")

        # --- CPU (NumPy) ---
        A_cpu = np.random.rand(n, n).astype(np.float32)
        B_cpu = np.random.rand(n, n).astype(np.float32)

        start = time.time()
        C_cpu = A_cpu @ B_cpu
        cpu_time = time.time() - start
        cpu_times.append(cpu_time)
        print(f"  CPU (NumPy):  {cpu_time:.4f} ç§’")

        # --- GPU (CuPy) ---
        A_gpu = cp.random.rand(n, n).astype(cp.float32)
        B_gpu = cp.random.rand(n, n).astype(cp.float32)

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›ã¯é…ã„ã“ã¨ãŒã‚ã‚‹ãŸã‚ï¼‰
        _ = A_gpu @ B_gpu
        cp.cuda.Stream.null.synchronize()

        start = time.time()
        C_gpu = A_gpu @ B_gpu
        cp.cuda.Stream.null.synchronize()  # GPUè¨ˆç®—ã®å®Œäº†ã‚’å¾…ã¤
        gpu_time = time.time() - start
        gpu_times.append(gpu_time)
        print(f"  GPU (CuPy):   {gpu_time:.4f} ç§’")

        # é«˜é€ŸåŒ–ç‡
        speedup = cpu_time / gpu_time
        speedups.append(speedup)
        print(f"  é«˜é€ŸåŒ–ç‡:     {speedup:.1f}x")

        # çµæœã®æ¤œè¨¼ï¼ˆè¨ˆç®—çµæœãŒæ­£ã—ã„ã‹ç¢ºèªï¼‰
        diff = np.abs(C_cpu - cp.asnumpy(C_gpu)).max()
        print(f"  æœ€å¤§èª¤å·®:     {diff:.2e}")

    # --- çµæœã®å¯è¦–åŒ– ---
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦å›³: è¨ˆç®—æ™‚é–“ã®æ¯”è¼ƒ
    ax = axes[0]
    x_pos = np.arange(len(sizes))
    width = 0.35

    ax.bar(x_pos - width / 2, cpu_times, width, label="CPU (NumPy)", color="steelblue")
    ax.bar(x_pos + width / 2, gpu_times, width, label="GPU (CuPy)", color="coral")

    ax.set_xlabel("Matrix Size", fontsize=12)
    ax.set_ylabel("Time (seconds)", fontsize=12)
    ax.set_title(
        "CPU vs GPU: Matrix Multiplication Time", fontsize=14, fontweight="bold"
    )
    ax.set_xticks(x_pos)
    ax.set_xticklabels([f"{s}Ã—{s}" for s in sizes])
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis="y")
    ax.set_yscale("log")  # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«

    # å³å›³: é«˜é€ŸåŒ–ç‡
    ax = axes[1]
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(sizes)))
    bars = ax.bar(range(len(sizes)), speedups, color=colors, edgecolor="black")

    ax.set_xlabel("Matrix Size", fontsize=12)
    ax.set_ylabel("Speedup (times)", fontsize=12)
    ax.set_title("GPU Speedup over CPU", fontsize=14, fontweight="bold")
    ax.set_xticks(range(len(sizes)))
    ax.set_xticklabels([f"{s}Ã—{s}" for s in sizes])
    ax.grid(True, alpha=0.3, axis="y")

    # å„æ£’ã«æ•°å€¤ã‚’è¡¨ç¤º
    for i, (bar, speedup) in enumerate(zip(bars, speedups)):
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{speedup:.1f}x",
            ha="center",
            va="bottom",
            fontsize=10,
            fontweight="bold",
        )

    plt.tight_layout()
    plt.savefig("exercise2_cpu_gpu_comparison.png", dpi=150, bbox_inches="tight")
    print("\nå›³ã‚’ 'exercise2_cpu_gpu_comparison.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
    plt.show()

    # --- çµæœã®ã‚µãƒãƒªãƒ¼ ---
    print("\nã€å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼ã€‘")
    print("-" * 70)
    print(
        f"æœ€å°é«˜é€ŸåŒ–ç‡: {min(speedups):.1f}x (è¡Œåˆ—ã‚µã‚¤ã‚º: {sizes[speedups.index(min(speedups))]}Ã—{sizes[speedups.index(min(speedups))]})"
    )
    print(
        f"æœ€å¤§é«˜é€ŸåŒ–ç‡: {max(speedups):.1f}x (è¡Œåˆ—ã‚µã‚¤ã‚º: {sizes[speedups.index(max(speedups))]}Ã—{sizes[speedups.index(max(speedups))]})"
    )
    print(
        "\nğŸ’¡ è€ƒå¯Ÿ: è¡Œåˆ—ã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨ãã¯GPUã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆãƒ‡ãƒ¼ã‚¿è»¢é€æ™‚é–“ç­‰ï¼‰ãŒ"
    )
    print("   ç›¸å¯¾çš„ã«å¤§ãããªã‚‹ãŸã‚ã€é«˜é€ŸåŒ–ç‡ãŒä½ããªã‚Šã¾ã™ã€‚")
    print("   è¡Œåˆ—ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã»ã©ã€GPUã®ä¸¦åˆ—è¨ˆç®—èƒ½åŠ›ãŒç™ºæ®ã•ã‚Œã€")
    print("   CPUã«å¯¾ã—ã¦åœ§å€’çš„ã«é«˜é€Ÿã«ãªã‚Šã¾ã™ã€‚")


# =====================================
# æ¼”ç¿’3: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’3: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®é–¢ä¿‚ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
- ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªã®é–¢ä¿‚ã‚’æ¨å®š
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3ç­‰ï¼‰ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’ç†è§£
"""
)


# --- ã‚·ãƒ³ãƒ—ãƒ«ãªå…¨çµåˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ---
class SimpleModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def count_parameters(model):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable


def estimate_memory(num_params, dtype="float32"):
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‹ã‚‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š"""
    bytes_per_param = {"float32": 4, "float16": 2, "int8": 1}
    bytes_size = bytes_per_param.get(dtype, 4)
    memory_bytes = num_params * bytes_size
    return memory_bytes / 1e6  # MBå˜ä½ã§è¿”ã™


# --- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦å®Ÿé¨“ ---
configs = [
    ("å°", 100, 256, 10),
    ("ä¸­", 1000, 1024, 100),
    ("å¤§", 10000, 4096, 1000),
]

print("\nã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‘")
print("-" * 70)

results = []

for name, input_size, hidden_size, output_size in configs:
    model = SimpleModel(input_size, hidden_size, output_size)
    total, trainable = count_parameters(model)

    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š
    memory_params = estimate_memory(total, "float32")
    memory_with_grads = memory_params * 2  # å‹¾é…ã‚‚ä¿å­˜ã™ã‚‹ãŸã‚

    results.append(
        {
            "name": name,
            "config": f"{input_size}â†’{hidden_size}â†’{output_size}",
            "params": total,
            "memory_params": memory_params,
            "memory_grads": memory_with_grads,
        }
    )

    print(f"\nãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {name}")
    print(f"  æ§‹æˆ:             {input_size} â†’ {hidden_size} â†’ {output_size}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:     {total:,}")
    print(f"  ãƒ¡ãƒ¢ãƒª (é‡ã¿ã®ã¿): {memory_params:.2f} MB")
    print(f"  ãƒ¡ãƒ¢ãƒª (å‹¾é…è¾¼ã¿): {memory_with_grads:.2f} MB")

# --- å®Ÿéš›ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®šï¼ˆGPUãŒã‚ã‚‹å ´åˆï¼‰ ---
if torch.cuda.is_available():
    print("\nã€å®Ÿéš›ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‘")

    for name, input_size, hidden_size, output_size in configs:
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

        model = SimpleModel(input_size, hidden_size, output_size).to(device)
        input_tensor = torch.randn(1, input_size).to(device)

        # Forward pass
        output = model(input_tensor)

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        allocated = torch.cuda.memory_allocated() / 1e6
        reserved = torch.cuda.memory_reserved() / 1e6
        peak = torch.cuda.max_memory_allocated() / 1e6

        print(f"\nãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {name}")
        print(f"  Allocated: {allocated:.2f} MB")
        print(f"  Reserved:  {reserved:.2f} MB")
        print(f"  Peak:      {peak:.2f} MB")

# --- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªæ¨å®š ---
print("\nã€æœ‰åãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã€‘")
print("-" * 70)

large_models = {
    "ResNet-50": 25.6e6,
    "BERT-Base": 110e6,
    "GPT-2": 1.5e9,
    "GPT-3": 175e9,
    "GPT-4 (æ¨å®š)": 1.7e12,
}

print(
    f"{'ãƒ¢ãƒ‡ãƒ«å':<20} {'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°':<15} {'ãƒ¡ãƒ¢ãƒª (FP32)':<15} {'ãƒ¡ãƒ¢ãƒª (FP16)':<15}"
)
print("-" * 70)

for name, params in large_models.items():
    memory_fp32 = params * 4 / 1e9  # GB
    memory_fp16 = params * 2 / 1e9  # GB
    print(
        f"{name:<20} {params / 1e9:>10.1f}B {memory_fp32:>12.1f} GB {memory_fp16:>12.1f} GB"
    )

print("\nğŸ’¡ è€ƒå¯Ÿ:")
print("   - GPT-3ã‚¯ãƒ©ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€é‡ã¿ã ã‘ã§700GBï¼ˆFP32ï¼‰å¿…è¦")
print("   - å­¦ç¿’æ™‚ã¯å‹¾é…ã‚„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹ã‚‚ä¿å­˜ã™ã‚‹ãŸã‚ã€ã•ã‚‰ã«2-4å€å¿…è¦")
print("   - ãã®ãŸã‚ã€è¤‡æ•°ã®GPUã«åˆ†æ•£ã—ã¦é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹")
print("   - FP16ã‚„Int8é‡å­åŒ–ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›å¯èƒ½")

# --- å¯è¦–åŒ– ---
fig, ax = plt.subplots(figsize=(10, 6))

model_names = list(large_models.keys())
params_billions = [large_models[name] / 1e9 for name in model_names]
memory_fp32_gb = [p * 4 / 1e9 for p in [large_models[name] for name in model_names]]

x_pos = np.arange(len(model_names))
bars = ax.bar(x_pos, memory_fp32_gb, color="steelblue", edgecolor="black")

ax.set_xlabel("Model", fontsize=12)
ax.set_ylabel("Memory (GB, FP32)", fontsize=12)
ax.set_title(
    "Memory Requirements for Large Language Models", fontsize=14, fontweight="bold"
)
ax.set_xticks(x_pos)
ax.set_xticklabels(model_names, rotation=15, ha="right")
ax.set_yscale("log")
ax.grid(True, alpha=0.3, axis="y")

# å„æ£’ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
for i, (bar, params) in enumerate(zip(bars, params_billions)):
    height = bar.get_height()
    ax.text(
        bar.get_x() + bar.get_width() / 2.0,
        height,
        f"{params:.1f}B",
        ha="center",
        va="bottom",
        fontsize=9,
    )

plt.tight_layout()
plt.savefig("exercise3_memory_usage.png", dpi=150, bbox_inches="tight")
print("\nå›³ã‚’ 'exercise3_memory_usage.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
plt.show()

# =====================================
# æ¼”ç¿’4: ä¸¦åˆ—è¨ˆç®—ã®åŠ¹æœå®Ÿæ„Ÿ
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’4: ä¸¦åˆ—è¨ˆç®—ã®åŠ¹æœå®Ÿæ„Ÿ")
print("=" * 70)

print(
    """
ã“ã®æ¼”ç¿’ã§ã¯ã€ä¸¦åˆ—åŒ–ã—ã‚„ã™ã„å‡¦ç†ã¨ä¸¦åˆ—åŒ–ã—ã«ãã„å‡¦ç†ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚
- ä¸¦åˆ—åŒ–å¯èƒ½: è¦ç´ ã”ã¨ã®ç‹¬ç«‹ã—ãŸæ¼”ç®—
- ä¸¦åˆ—åŒ–å›°é›£: å‰ã®çµæœã«ä¾å­˜ã™ã‚‹å†å¸°çš„ãªè¨ˆç®—
"""
)


def parallel_friendly(n):
    """ä¸¦åˆ—åŒ–ã—ã‚„ã™ã„å‡¦ç†: è¦ç´ ã”ã¨ã®ç©ï¼ˆå„è¨ˆç®—ãŒç‹¬ç«‹ï¼‰"""
    A = np.random.rand(n, n)
    B = np.random.rand(n, n)

    start = time.time()
    C = A * B  # è¦ç´ ã”ã¨ã®ç©ï¼ˆä¸¦åˆ—åŒ–å¯èƒ½ï¼‰
    elapsed = time.time() - start

    return elapsed


def parallel_unfriendly(n):
    """ä¸¦åˆ—åŒ–ã—ã«ãã„å‡¦ç†: Fibonacciæ•°åˆ—ï¼ˆå‰ã®çµæœã«ä¾å­˜ï¼‰"""
    result = np.zeros(n)
    result[0] = 1
    result[1] = 1

    start = time.time()
    for i in range(2, n):
        result[i] = result[i - 1] + result[i - 2]  # å‰ã®çµæœã«ä¾å­˜
    elapsed = time.time() - start

    return elapsed


# --- å®Ÿé¨“ ---
sizes = [1000, 5000, 10000, 50000]  # , 100000

parallel_times = []
sequential_times = []
ratios = []

print("\nã€ä¸¦åˆ—åŒ–å¯èƒ½ vs ä¸¦åˆ—åŒ–å›°é›£ãªå‡¦ç†ã®æ¯”è¼ƒã€‘")
print("-" * 70)
print(f"{'ã‚µã‚¤ã‚º':<12} {'ä¸¦åˆ—åŒ–å¯èƒ½ (ms)':<18} {'é€æ¬¡å‡¦ç† (ms)':<18} {'æ¯”ç‡':<10}")
print("-" * 70)

for n in sizes:
    t_parallel = parallel_friendly(n) * 1000  # ãƒŸãƒªç§’ã«å¤‰æ›
    t_sequential = parallel_unfriendly(n) * 1000
    ratio = t_sequential / t_parallel if t_parallel > 0 else 0

    parallel_times.append(t_parallel)
    sequential_times.append(t_sequential)
    ratios.append(ratio)

    print(f"{n:<12} {t_parallel:<18.2f} {t_sequential:<18.2f} {ratio:<10.1f}x")

# --- å¯è¦–åŒ– ---
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å·¦å›³: å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ
ax = axes[0]
x_pos = np.arange(len(sizes))
width = 0.35

ax.bar(
    x_pos - width / 2,
    parallel_times,
    width,
    label="Parallel-friendly",
    color="green",
    alpha=0.7,
)
ax.bar(
    x_pos + width / 2,
    sequential_times,
    width,
    label="Sequential",
    color="red",
    alpha=0.7,
)

ax.set_xlabel("Data Size", fontsize=12)
ax.set_ylabel("Time (milliseconds)", fontsize=12)
ax.set_title(
    "Execution Time: Parallel-friendly vs Sequential", fontsize=14, fontweight="bold"
)
ax.set_xticks(x_pos)
ax.set_xticklabels([f"{s:,}" for s in sizes], rotation=15, ha="right")
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis="y")
ax.set_yscale("log")

# å³å›³: å®Ÿè¡Œæ™‚é–“ã®æ¯”ç‡
ax = axes[1]
ax.plot(sizes, ratios, "o-", linewidth=2, markersize=8, color="purple")

ax.set_xlabel("Data Size", fontsize=12)
ax.set_ylabel("Time Ratio (Sequential / Parallel)", fontsize=12)
ax.set_title("Performance Gap", fontsize=14, fontweight="bold")
ax.grid(True, alpha=0.3)

# ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«å€¤ã‚’è¡¨ç¤º
for i, (size, ratio) in enumerate(zip(sizes, ratios)):
    ax.text(size, ratio, f"{ratio:.1f}x", ha="center", va="bottom", fontsize=9)

plt.tight_layout()
plt.savefig("exercise4_parallelization.png", dpi=150, bbox_inches="tight")
print("\nå›³ã‚’ 'exercise4_parallelization.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ")
plt.show()

print("\nğŸ’¡ è€ƒå¯Ÿ:")
print("   - è¦ç´ ã”ã¨ã®æ¼”ç®—ï¼ˆA * Bï¼‰ã¯å„è¦ç´ ãŒç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€")
print("     NumPyã¯å†…éƒ¨ã§è‡ªå‹•çš„ã«ä¸¦åˆ—åŒ–ï¼ˆSIMDå‘½ä»¤ç­‰ï¼‰ã‚’è¡Œã„ã¾ã™")
print("   - Fibonacciæ•°åˆ—ã®ã‚ˆã†ãªå†å¸°çš„ãªè¨ˆç®—ã¯ã€å‰ã®çµæœã«ä¾å­˜ã™ã‚‹ãŸã‚ã€")
print("     ä¸¦åˆ—åŒ–ãŒå›°é›£ã§ã€é€æ¬¡çš„ã«è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
print("   - GPUã¯ã“ã®ã‚ˆã†ãªã€Œå¤§é‡ã®ç‹¬ç«‹ã—ãŸè¨ˆç®—ã€ã‚’å¾—æ„ã¨ã—ã¦ã„ã¾ã™")

print("\n" + "=" * 70)
print("æ¼”ç¿’4 å®Œäº†")
print("=" * 70)

# =====================================
# æ¼”ç¿’ã®ã¾ã¨ã‚
# =====================================

print("\n" + "=" * 70)
print("æ¼”ç¿’ã®ã¾ã¨ã‚")
print("=" * 70)

print(
    """
æœ¬æ¼”ç¿’ã§å­¦ã‚“ã ã“ã¨:

1. ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±ã®ç¢ºèª
   - GPUã€CPUã€ãƒ¡ãƒ¢ãƒªã®ä»•æ§˜ã‚’ç¢ºèª
   - è‡ªåˆ†ãŒä½¿ã£ã¦ã„ã‚‹è¨ˆç®—ç’°å¢ƒã‚’ç†è§£ã™ã‚‹é‡è¦æ€§

2. CPU vs GPU é€Ÿåº¦æ¯”è¼ƒ
   - GPUã¯å¤§è¦æ¨¡ãªè¡Œåˆ—æ¼”ç®—ã§åœ§å€’çš„ã«é«˜é€Ÿ
   - å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯CPUã®æ–¹ãŒé€Ÿã„å ´åˆã‚‚ã‚ã‚‹
   - ãƒ‡ãƒ¼ã‚¿è»¢é€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹

3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¦³å¯Ÿ
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ¯”ä¾‹ã™ã‚‹
   - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã¯è†¨å¤§ãªãƒ¡ãƒ¢ãƒªãŒå¿…è¦
   - FP16ã‚„Int8é‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›å¯èƒ½
   - å­¦ç¿’æ™‚ã¯æ¨è«–æ™‚ã®2-4å€ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦

4. ä¸¦åˆ—è¨ˆç®—ã®åŠ¹æœ
   - ç‹¬ç«‹ã—ãŸè¨ˆç®—ã¯ä¸¦åˆ—åŒ–ã—ã‚„ã™ã„
   - ä¾å­˜é–¢ä¿‚ã®ã‚ã‚‹è¨ˆç®—ã¯ä¸¦åˆ—åŒ–ãŒå›°é›£
   - GPUã¯ä¸¦åˆ—åŒ–å¯èƒ½ãªè¨ˆç®—ã‚’å¤§é‡ã«å‡¦ç†ã§ãã‚‹

ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
- AIãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¨è«–ã«ã¯å¤§è¦æ¨¡ãªè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦
- GPUã¯è¡Œåˆ—æ¼”ç®—ã«ç‰¹åŒ–ã—ãŸä¸¦åˆ—è¨ˆç®—è£…ç½®
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã¯æ•°åƒï½æ•°ä¸‡ã®GPUã‚’ä½¿ç”¨
- ãƒ¡ãƒ¢ãƒªã‚‚CPUã¨åŒæ§˜ã«é‡è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ç†è§£ã¯AIé–‹ç™ºã«ä¸å¯æ¬ 

ã€æ¬¡å›ã«å‘ã‘ã¦ã€‘
æ¬¡å›ã¯ã€ã“ã‚Œã‚‰ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚’æ´»ç”¨ã—ãŸå®Ÿéš›ã®AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã™ã€‚
"""
)

print("\næ¼”ç¿’è³‡æ–™ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ")
print("\nç”Ÿæˆã•ã‚ŒãŸå›³:")
if GPU_AVAILABLE:
    print("  - exercise2_cpu_gpu_comparison.png")
print("  - exercise3_memory_usage.png")
print("  - exercise4_parallelization.png")

print("\n" + "=" * 70)
print("ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")
print("=" * 70)
