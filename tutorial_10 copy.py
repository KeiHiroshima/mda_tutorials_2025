"""
MDAå…¥é–€ 2025å¹´åº¦ ç¬¬10å›æ¼”ç¿’ï¼ˆä¿®æ­£ç‰ˆï¼‰
å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°

æ¼”ç¿’å†…å®¹:
1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®å®Ÿè·µï¼ˆZero-shot / Few-shotï¼‰
2. In-Context Learningã®ä½“é¨“
3. Chain-of-Thought (CoT) Promptingã®å®Ÿé¨“

ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆå„ªå…ˆé †ä½é †ï¼‰:
1. Qwen/Qwen2.5-0.5B-Instructï¼ˆæœ€å„ªå…ˆï¼‰
2. llm-jp/llm-jp-3-1.8b-instruct
3. rinna/japanese-gpt2-medium
4. cyberagent/open-calm-smallï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:
- JGLUE/MARC-jaï¼ˆæ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
- tyqiangz/multilingual-sentimentsï¼ˆå¤šè¨€èªæ„Ÿæƒ…åˆ†æï¼‰
"""

# ============================================
# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
# ============================================

print("=" * 70)
print("ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
print("=" * 70)

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
#!pip install -q transformers torch datasets accelerate sentencepiece

import gc
import random
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

warnings.filterwarnings("ignore")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
plt.rcParams["font.sans-serif"] = ["DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


# ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


set_seed(42)

print("\nâœ“ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
print("=" * 70)
print("MDAå…¥é–€ ç¬¬10å›æ¼”ç¿’: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°")
print("=" * 70)

# ============================================
# å…¬é–‹LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’é †æ¬¡è©¦è¡Œï¼‰
# ============================================

print("\n" + "=" * 70)
print("å…¬é–‹LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿")
print("=" * 70)

# è©¦è¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
model_candidates = [
    {
        "name": "Qwen/Qwen2.5-0.5B-Instruct",
        "params": "500M",
        "description": "Alibabaé–‹ç™ºã€å¤šè¨€èªå¯¾å¿œã€Instructionãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿",
    },
    {
        "name": "llm-jp/llm-jp-3-1.8b-instruct",
        "params": "1.8B",
        "description": "å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€é–‹ç™ºã€æ—¥æœ¬èªç‰¹åŒ–",
    },
    {
        "name": "rinna/japanese-gpt2-medium",
        "params": "330M",
        "description": "æ—¥æœ¬èªGPT-2ã€å®Ÿç¸¾è±Šå¯Œ",
    },
    {
        "name": "cyberagent/open-calm-small",
        "params": "160M",
        "description": "ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™ºã€è¶…è»½é‡",
    },
]

# ãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if device == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(
        f"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
    )

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ
model = None
tokenizer = None
model_name = None
loaded_model_info = None

print("\n" + "-" * 70)
print("ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰è©¦è¡Œï¼ˆå„ªå…ˆé †ä½é †ï¼‰")
print("-" * 70)

for i, candidate in enumerate(model_candidates, 1):
    model_name = candidate["name"]
    print(f"\nã€è©¦è¡Œ {i}/4ã€‘{model_name}")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {candidate['params']}")
    print(f"  ç‰¹å¾´: {candidate['description']}")
    print("  èª­ã¿è¾¼ã¿ä¸­...", end="")

    try:
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True,
        )

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª
        num_params = sum(p.numel() for p in model.parameters()) / 1e6

        print(" âœ“ æˆåŠŸï¼")
        print(f"  å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {num_params:.0f}M")

        loaded_model_info = candidate
        break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

    except Exception as e:
        print(" âœ— å¤±æ•—")
        print(f"  ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}...")

        # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
        if model is not None:
            del model
        if tokenizer is not None:
            del tokenizer
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

        model = None
        tokenizer = None

        if i < len(model_candidates):
            print("  â†’ æ¬¡ã®å€™è£œãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™...")
        else:
            print("  â†’ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")

# ãƒ­ãƒ¼ãƒ‰çµæœã®ç¢ºèª
if model is None or tokenizer is None:
    raise RuntimeError(
        "âŒ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Google Colabã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )

print("\n" + "=" * 70)
print(f"âœ“ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {loaded_model_info['params']}")
print(f"  ç‰¹å¾´: {loaded_model_info['description']}")
print("=" * 70)

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)  # device=0 if device == "cuda" else -1,

print("\nâœ“ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æº–å‚™å®Œäº†")

# ============================================
# å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
# ============================================

print("\n" + "=" * 70)
print("å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿")
print("=" * 70)

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå€™è£œï¼ˆå„ªå…ˆé †ä½é †ï¼‰
dataset_candidates = [
    {
        "name": "tyqiangz/multilingual-sentiments",
        "config": "japanese",
        "description": "å¤šè¨€èªæ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ—¥æœ¬èªï¼‰",
    },
    {
        "name": "shunk031/JGLUE",
        "config": "MARC-ja",
        "description": "æ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆAmazonï¼‰",
    },
]

dataset = None
dataset_info = None

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
if dataset is None:
    print("ä»£æ›¿ã¨ã—ã¦æ‰‹å‹•ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™\n")

    # ä»£æ›¿ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥æœ¬èªæ„Ÿæƒ…åˆ†æï¼‰
    dataset = [
        {"text": "ã“ã®å•†å“ã¯æœ€é«˜ã§ã™ï¼è²·ã£ã¦è‰¯ã‹ã£ãŸã§ã™ã€‚", "label": "positive"},
        {"text": "æœŸå¾…å¤–ã‚Œã§ã—ãŸã€‚å“è³ªãŒæ‚ªã™ãã¾ã™ã€‚", "label": "negative"},
        {"text": "å€¤æ®µã®å‰²ã«ã¯æ™®é€šã§ã™ã€‚å¯ã‚‚ãªãä¸å¯ã‚‚ãªãã€‚", "label": "neutral"},
        {
            "text": "ç´ æ™´ã‚‰ã—ã„è£½å“ã§ã™ã€‚å‹äººã«ã‚‚å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã€‚",
            "label": "positive",
        },
        {"text": "ã‚µãƒ¼ãƒ“ã‚¹ãŒæœ€æ‚ªã§ã—ãŸã€‚äºŒåº¦ã¨åˆ©ç”¨ã—ã¾ã›ã‚“ã€‚", "label": "negative"},
        {"text": "æ©Ÿèƒ½ã¯è‰¯ã„ã§ã™ãŒã€ãƒ‡ã‚¶ã‚¤ãƒ³ãŒã‚¤ãƒã‚¤ãƒã§ã™ã€‚", "label": "neutral"},
        {"text": "æœŸå¾…ä»¥ä¸Šã®æ€§èƒ½ã§ã—ãŸã€‚å¤§æº€è¶³ã§ã™ï¼", "label": "positive"},
        {"text": "èª¬æ˜ã¨å®Ÿç‰©ãŒå…¨ç„¶é•ã†ã€‚è©æ¬ºãƒ¬ãƒ™ãƒ«ã§ã™ã€‚", "label": "negative"},
        {"text": "ã“ã®ä¾¡æ ¼ãªã‚‰å¦¥å½“ã ã¨æ€ã„ã¾ã™ã€‚", "label": "neutral"},
        {"text": "æ„Ÿå‹•ã—ã¾ã—ãŸã€‚æœ¬å½“ã«è²·ã£ã¦è‰¯ã‹ã£ãŸã§ã™ã€‚", "label": "positive"},
    ]

    dataset_info = {
        "name": "æ‰‹å‹•ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿",
        "description": "æ—¥æœ¬èªå•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼æ„Ÿæƒ…åˆ†æ",
    }

    print(f"âœ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†ï¼ˆ{len(dataset)}ä»¶ï¼‰")

else:
    print(f"\nâœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {dataset_info['name']}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    print("\nã€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã€‘")
    sample = dataset[0] if isinstance(dataset, list) else dataset[0]
    for key, value in sample.items():
        if isinstance(value, str) and len(value) > 100:
            print(f"  {key}: {value[:100]}...")
        else:
            print(f"  {key}: {value}")

# ============================================
# æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤å®Ÿè·µ")
print("=" * 70)

"""
ã“ã®æ¼”ç¿’ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆãŒå‡ºåŠ›ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ã‚’å­¦ã³ã¾ã™ã€‚
- Zero-shot promptingï¼ˆä¾‹é¡Œãªã—ï¼‰
- Few-shot promptingï¼ˆä¾‹é¡Œã‚ã‚Šï¼‰
ã®é•ã„ã‚’å®Ÿéš›ã«ç¢ºèªã—ã¾ã™ã€‚
"""

# --- ã‚¿ã‚¹ã‚¯1: ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æ ---
print("\n" + "-" * 70)
print("ã‚¿ã‚¹ã‚¯1: ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æ")
print("-" * 70)

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ
if isinstance(dataset, list):
    test_text = dataset[0]["text"]
    test_label = dataset[0]["label"]
else:
    test_idx = 50
    sample = dataset[test_idx]
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
    if "text" in sample:
        test_text = sample["text"]
    elif "sentence" in sample:
        test_text = sample["sentence"]
    elif "review" in sample:
        test_text = sample["review"]
    else:
        test_text = str(list(sample.values())[0])

    if "label" in sample:
        test_label = sample["label"]
    elif "sentiment" in sample:
        test_label = sample["sentiment"]
    else:
        test_label = "unknown"

print("\nã€ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‘")
print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
print(f"æ­£è§£ãƒ©ãƒ™ãƒ«: {test_label}")

# Zero-shot prompting
print("\nã€å®Ÿé¨“1-1ã€‘Zero-shot Prompting")
print("-" * 50)

zero_shot_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}

æ„Ÿæƒ…:"""

print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
print(zero_shot_prompt[:150] + "...")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
try:
    output_zero = generator(
        zero_shot_prompt,
        max_new_tokens=20,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )

    generated_text = output_zero[0]["generated_text"]
    summary_zero = generated_text[len(zero_shot_prompt) :].strip()

    print("\nç”Ÿæˆã•ã‚ŒãŸåˆ†é¡çµæœ:")
    print(f"{summary_zero[:50]}")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    summary_zero = "ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"

# Few-shot prompting
print("\n\nã€å®Ÿé¨“1-2ã€‘Few-shot Promptingï¼ˆä¾‹é¡Œä»˜ãï¼‰")
print("-" * 50)

# ä¾‹é¡Œã‚’ä½œæˆ
if isinstance(dataset, list):
    examples = dataset[1:4]
else:
    examples = [dataset[i] for i in [10, 20, 30]]

few_shot_prompt = "ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œã‚’è¿½åŠ 
for i, example in enumerate(examples[:3], 1):
    if isinstance(dataset, list):
        ex_text = example["text"]
        ex_label = example["label"]
    else:
        if "text" in example:
            ex_text = example["text"]
        elif "sentence" in example:
            ex_text = example["sentence"]
        elif "review" in example:
            ex_text = example["review"]
        else:
            ex_text = str(list(example.values())[0])[:100]

        if "label" in example:
            ex_label = example["label"]
        elif "sentiment" in example:
            ex_label = example["sentiment"]
        else:
            ex_label = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if i % 2 == 0 else "ãƒã‚¬ãƒ†ã‚£ãƒ–"

    few_shot_prompt += f"ä¾‹{i}:\nãƒ†ã‚­ã‚¹ãƒˆ: {ex_text[:80]}...\næ„Ÿæƒ…: {ex_label}\n\n"

# ãƒ†ã‚¹ãƒˆæ–‡ã‚’è¿½åŠ 
few_shot_prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}\n\næ„Ÿæƒ…:"

print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¾‹é¡Œ3ã¤ä»˜ãï¼‰:")
print(few_shot_prompt[:200] + "...")

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
try:
    output_few = generator(
        few_shot_prompt,
        max_new_tokens=20,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )

    generated_text_few = output_few[0]["generated_text"]
    summary_few = generated_text_few[len(few_shot_prompt) :].strip()

    print("\nç”Ÿæˆã•ã‚ŒãŸåˆ†é¡çµæœ:")
    print(f"{summary_few[:50]}")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    summary_few = "ï¼ˆç”Ÿæˆå¤±æ•—ï¼‰"

# æ¯”è¼ƒ
print("\n" + "=" * 70)
print("ã€æ¯”è¼ƒçµæœã€‘")
print("=" * 70)
print(f"æ­£è§£:       {test_label}")
print(f"Zero-shot:  {summary_zero[:30]}")
print(f"Few-shot:   {summary_few[:30]}")
print(
    "\nğŸ’¡ Few-shotã§ã¯ä¾‹é¡Œã‹ã‚‰å½¢å¼ã‚„ã‚«ãƒ†ã‚´ãƒªã‚’å­¦ç¿’ã—ã€ã‚ˆã‚Šé©åˆ‡ãªåˆ†é¡ã«ãªã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™"
)

# ============================================
# æ¼”ç¿’2: In-Context Learningã®ä½“é¨“
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’2: In-Context Learningã®ä½“é¨“")
print("=" * 70)

print("""
ä¾‹é¡Œã®æ•°ã‚’å¤‰ãˆã‚‹ã“ã¨ã§æ€§èƒ½ãŒã©ã†å¤‰åŒ–ã™ã‚‹ã‹ã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
ã‚¿ã‚¹ã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†é¡
""")

# --- å®Ÿé¨“: ä¾‹é¡Œæ•°ã‚’å¤‰ãˆã¦åˆ†é¡ ---
print("\n" + "-" * 70)
print("å®Ÿé¨“: ä¾‹é¡Œæ•°ã¨åˆ†é¡ç²¾åº¦ã®é–¢ä¿‚")
print("-" * 70)


def create_sentiment_prompt(num_examples, test_text, dataset):
    """æ„Ÿæƒ…åˆ†é¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
    prompt = "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"

    if num_examples > 0:
        # ä¾‹é¡Œã‚’è¿½åŠ 
        examples_to_use = min(num_examples, 5)
        if isinstance(dataset, list):
            examples = dataset[:examples_to_use]
        else:
            examples = [dataset[i * 10] for i in range(examples_to_use)]

        for i, example in enumerate(examples, 1):
            if isinstance(dataset, list):
                ex_text = example["text"][:60]
                ex_label = example["label"]
            else:
                if "text" in example:
                    ex_text = example["text"][:60]
                elif "review" in example:
                    ex_text = example["review"][:60]
                else:
                    ex_text = str(list(example.values())[0])[:60]

                if "label" in example:
                    ex_label = example["label"]
                else:
                    ex_label = "ãƒã‚¸ãƒ†ã‚£ãƒ–" if i % 2 == 0 else "ãƒã‚¬ãƒ†ã‚£ãƒ–"

            prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {ex_text}...\næ„Ÿæƒ…: {ex_label}\n\n"

    # ãƒ†ã‚¹ãƒˆæ–‡ã‚’è¿½åŠ 
    prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}\næ„Ÿæƒ…:"
    return prompt


# ãƒ†ã‚¹ãƒˆç”¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ï¼‰
if isinstance(dataset, list):
    test_sample = dataset[5]
    icl_test_text = test_sample["text"]
    icl_test_label = test_sample["label"]
else:
    icl_test_idx = 100
    icl_sample = dataset[icl_test_idx]
    if "text" in icl_sample:
        icl_test_text = icl_sample["text"]
    elif "review" in icl_sample:
        icl_test_text = icl_sample["review"]
    else:
        icl_test_text = "ã“ã®è£½å“ã¯æœŸå¾…ä»¥ä¸Šã®æ€§èƒ½ã§ã—ãŸã€‚"

    if "label" in icl_sample:
        icl_test_label = icl_sample["label"]
    else:
        icl_test_label = "ãƒã‚¸ãƒ†ã‚£ãƒ–"

print("\nã€ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‘")
print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {icl_test_text}")
print(f"æ­£è§£: {icl_test_label}")

# Zero-shotï¼ˆä¾‹é¡Œ0å€‹ï¼‰
print("\nã€Zero-shotï¼ˆä¾‹é¡Œãªã—ï¼‰ã€‘")
prompt_0shot = create_sentiment_prompt(0, icl_test_text, dataset)

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{prompt_0shot[:150]}...")

try:
    output = generator(
        prompt_0shot,
        max_new_tokens=15,
        num_return_sequences=1,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )
    result_0shot = output[0]["generated_text"][len(prompt_0shot) :].strip()
    print(f"äºˆæ¸¬æ„Ÿæƒ…: {result_0shot[:30]}")
except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_0shot = "ã‚¨ãƒ©ãƒ¼"

# Few-shotï¼ˆä¾‹é¡Œ3å€‹ï¼‰
print("\nã€Few-shotï¼ˆä¾‹é¡Œ3å€‹ï¼‰ã€‘")
prompt_3shot = create_sentiment_prompt(3, icl_test_text, dataset)

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¾‹é¡Œã‚ã‚Šï¼‰:\n{prompt_3shot[:200]}...")

try:
    output = generator(
        prompt_3shot,
        max_new_tokens=15,
        num_return_sequences=1,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )
    result_3shot = output[0]["generated_text"][len(prompt_3shot) :].strip()
    print(f"äºˆæ¸¬æ„Ÿæƒ…: {result_3shot[:30]}")
except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_3shot = "ã‚¨ãƒ©ãƒ¼"

print("\n" + "=" * 70)
print("ã€çµæœæ¯”è¼ƒã€‘")
print("=" * 70)
print(f"æ­£è§£:         {icl_test_label}")
print(f"Zero-shotäºˆæ¸¬: {result_0shot[:30]}")
print(f"Few-shotäºˆæ¸¬:  {result_3shot[:30]}")
print("\nğŸ’¡ ä¾‹é¡ŒãŒã‚ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚¿ã‚¹ã‚¯ã®å½¢å¼ã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç†è§£ã§ãã¾ã™")

# ============================================
# æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’3: Chain-of-Thought (CoT) Prompting")
print("=" * 70)

print("""
æ€è€ƒéç¨‹ã‚’ä¾‹é¡Œã«å«ã‚ã‚‹ã“ã¨ã§ã€æ¨è«–èƒ½åŠ›ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
ã‚¿ã‚¹ã‚¯: ç°¡å˜ãªç®—æ•°ã®æ–‡ç« é¡Œ
""")

# --- ç®—æ•°æ–‡ç« é¡Œã®ã‚µãƒ³ãƒ—ãƒ« ---
math_problems = [
    {
        "question": "å¤ªéƒã¯5å€‹ã®ã‚Šã‚“ã”ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚3å€‹ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "answer": "8å€‹",
        "reasoning": "æœ€åˆã«5å€‹æŒã£ã¦ã„ã¦ã€3å€‹ã‚‚ã‚‰ã£ãŸã®ã§ã€5 + 3 = 8å€‹ã§ã™ã€‚",
    },
    {
        "question": "èŠ±å­ã¯10å€‹ã®ã¿ã‹ã‚“ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚4å€‹é£Ÿã¹ã¾ã—ãŸã€‚ä»Šä½•å€‹ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "answer": "6å€‹",
        "reasoning": "æœ€åˆã«10å€‹æŒã£ã¦ã„ã¦ã€4å€‹é£Ÿã¹ãŸã®ã§ã€10 - 4 = 6å€‹ã§ã™ã€‚",
    },
    {
        "question": "ã‚¯ãƒ©ã‚¹ã«30äººã®ç”Ÿå¾’ãŒã„ã¾ã™ã€‚ãã®ã†ã¡12äººãŒç”·å­ã§ã™ã€‚å¥³å­ã¯ä½•äººã§ã™ã‹ï¼Ÿ",
        "answer": "18äºº",
        "reasoning": "å…¨ä½“ãŒ30äººã§ã€ç”·å­ãŒ12äººãªã®ã§ã€å¥³å­ã¯ 30 - 12 = 18äººã§ã™ã€‚",
    },
]

# ãƒ†ã‚¹ãƒˆå•é¡Œ
test_problem = {
    "question": "æœ¬å±‹ã§500å††ã®æœ¬ã‚’2å†Šã¨300å††ã®é›‘èªŒã‚’1å†Šè²·ã„ã¾ã—ãŸã€‚åˆè¨ˆé‡‘é¡ã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
    "answer": "1300å††",
}

print("\nã€ãƒ†ã‚¹ãƒˆå•é¡Œã€‘")
print(f"å•é¡Œ: {test_problem['question']}")
print(f"æ­£è§£: {test_problem['answer']}")

# --- å®Ÿé¨“1: é€šå¸¸ã®Few-shot ---
print("\n" + "-" * 70)
print("å®Ÿé¨“1: é€šå¸¸ã®Few-shotï¼ˆæ€è€ƒéç¨‹ãªã—ï¼‰")
print("-" * 70)

standard_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œï¼ˆç­”ãˆã®ã¿ï¼‰
for i, example in enumerate(math_problems[:2], 1):
    standard_prompt += f"å•é¡Œ: {example['question']}\n"
    standard_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

# ãƒ†ã‚¹ãƒˆå•é¡Œ
standard_prompt += f"å•é¡Œ: {test_problem['question']}\nç­”ãˆ:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{standard_prompt}")

try:
    output_standard = generator(
        standard_prompt,
        max_new_tokens=30,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )

    result_standard = output_standard[0]["generated_text"][
        len(standard_prompt) :
    ].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸç­”ãˆ: {result_standard[:50]}")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_standard = "ã‚¨ãƒ©ãƒ¼"

# --- å®Ÿé¨“2: CoT Few-shot ---
print("\n" + "-" * 70)
print("å®Ÿé¨“2: CoT Few-shotï¼ˆæ€è€ƒéç¨‹ã‚ã‚Šï¼‰")
print("-" * 70)

cot_prompt = "ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’ã€è¨ˆç®—éç¨‹ã‚’ç¤ºã—ãªãŒã‚‰è§£ã„ã¦ãã ã•ã„ã€‚\n\n"

# ä¾‹é¡Œï¼ˆæ€è€ƒéç¨‹ä»˜ãï¼‰
for i, example in enumerate(math_problems[:2], 1):
    cot_prompt += f"å•é¡Œ: {example['question']}\n"
    cot_prompt += f"è€ƒãˆæ–¹: {example['reasoning']}\n"
    cot_prompt += f"ç­”ãˆ: {example['answer']}\n\n"

# ãƒ†ã‚¹ãƒˆå•é¡Œ
cot_prompt += f"å•é¡Œ: {test_problem['question']}\nè€ƒãˆæ–¹:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{cot_prompt}")

try:
    output_cot = generator(
        cot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )

    result_cot = output_cot[0]["generated_text"][len(cot_prompt) :].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹ã¨ç­”ãˆ: {result_cot[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_cot = "ã‚¨ãƒ©ãƒ¼"

# --- å®Ÿé¨“3: Zero-shot CoT ---
print("\n" + "-" * 70)
print("å®Ÿé¨“3: Zero-shot CoTï¼ˆã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ ï¼‰")
print("-" * 70)

zero_shot_cot_prompt = f"""ä»¥ä¸‹ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„ã€‚æ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€‚

å•é¡Œ: {test_problem["question"]}

è€ƒãˆæ–¹:"""

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{zero_shot_cot_prompt}")

try:
    output_zero_cot = generator(
        zero_shot_cot_prompt,
        max_new_tokens=50,
        num_return_sequences=1,
        temperature=0.3,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )

    result_zero_cot = output_zero_cot[0]["generated_text"][
        len(zero_shot_cot_prompt) :
    ].strip()
    print(f"\nç”Ÿæˆã•ã‚ŒãŸæ€è€ƒéç¨‹: {result_zero_cot[:100]}...")

except Exception as e:
    print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    result_zero_cot = "ã‚¨ãƒ©ãƒ¼"

# çµæœæ¯”è¼ƒ
print("\n" + "=" * 70)
print("ã€CoTåŠ¹æœã®æ¯”è¼ƒã€‘")
print("=" * 70)
print(f"æ­£è§£: {test_problem['answer']}")
print(f"\né€šå¸¸Few-shot:    {result_standard[:50]}")
print(f"CoT Few-shot:    {result_cot[:50]}")
print(f"Zero-shot CoT:   {result_zero_cot[:50]}")
print("\nğŸ’¡ CoTã§ã¯æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ­£ç¢ºãªæ¨è«–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")

# ============================================
# æ¼”ç¿’4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µ
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å®Ÿè·µ")
print("=" * 70)

print("""
åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®åŸå‰‡ã‚’å®Ÿè·µã—ã¾ã™ã€‚
åŒã˜ã‚¿ã‚¹ã‚¯ã§ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹ã§çµæœãŒå¤§ããå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
""")

# ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
prac_text = "é›°å›²æ°—ã¯è‰¯ã‹ã£ãŸã§ã™ãŒã€æ–™ç†ãŒå†·ã‚ã¦ã„ã¦æ®‹å¿µã§ã—ãŸã€‚"

print("\nã€ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã€‘")
print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {prac_text}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ›–æ˜§ãªæŒ‡ç¤º
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ›–æ˜§ãªæŒ‡ç¤º")
print("-" * 70)

vague_prompt = f"æ„Ÿæƒ…ã‚’æ•™ãˆã¦ã€‚\n\n{prac_text}\n\næ„Ÿæƒ…:"
print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{vague_prompt}")

try:
    output_vague = generator(
        vague_prompt,
        max_new_tokens=20,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )
    result_vague = output_vague[0]["generated_text"][len(vague_prompt) :].strip()
    print(f"\nçµæœ: {result_vague[:30]}")
except Exception as e:
    result_vague = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ˜ç¢ºãªæŒ‡ç¤º + å½¢å¼æŒ‡å®š
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ˜ç¢ºãªæŒ‡ç¤º + å‡ºåŠ›å½¢å¼ã®æŒ‡å®š")
print("-" * 70)

clear_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
æ„Ÿæƒ…ã®ã¿ã‚’ç­”ãˆã€èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {prac_text}

æ„Ÿæƒ…:"""

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:\n{clear_prompt}")

try:
    output_clear = generator(
        clear_prompt,
        max_new_tokens=10,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )
    result_clear = output_clear[0]["generated_text"][len(clear_prompt) :].strip()
    print(f"\nçµæœ: {result_clear[:20]}")
except Exception as e:
    result_clear = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: å½¹å‰²è¨­å®š + Few-shot + å½¢å¼æŒ‡å®š
print("\n" + "-" * 70)
print("ãƒ‘ã‚¿ãƒ¼ãƒ³3: å½¹å‰²è¨­å®š + Few-shot + å½¢å¼æŒ‡å®š")
print("-" * 70)

best_prompt = """ã‚ãªãŸã¯æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã§åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ä¾‹:
ãƒ†ã‚­ã‚¹ãƒˆ: ã“ã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®æ–™ç†ã¯æœ€é«˜ã§ã—ãŸï¼ã¾ãŸæ¥ãŸã„ã§ã™ã€‚
æ„Ÿæƒ…: ãƒã‚¸ãƒ†ã‚£ãƒ–

ãƒ†ã‚­ã‚¹ãƒˆ: ã‚µãƒ¼ãƒ“ã‚¹ãŒæ‚ªãã¦äºŒåº¦ã¨è¡ŒããŸããªã„ã§ã™ã€‚
æ„Ÿæƒ…: ãƒã‚¬ãƒ†ã‚£ãƒ–

ãƒ†ã‚­ã‚¹ãƒˆ: æ–™ç†ã¯ç¾å‘³ã—ã„ã‘ã©ã€å€¤æ®µãŒé«˜ã™ãã¾ã™ã€‚
æ„Ÿæƒ…: ä¸­ç«‹

"""
best_prompt += f"ãƒ†ã‚­ã‚¹ãƒˆ: {prac_text}\næ„Ÿæƒ…:"

print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä¸€éƒ¨çœç•¥ï¼‰:\n{best_prompt[:200]}...")

try:
    output_best = generator(
        best_prompt,
        max_new_tokens=10,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
        if tokenizer.eos_token_id
        else tokenizer.pad_token_id,
        truncation=True,
    )
    result_best = output_best[0]["generated_text"][len(best_prompt) :].strip()
    print(f"\nçµæœ: {result_best[:20]}")
except Exception as e:
    result_best = "ã‚¨ãƒ©ãƒ¼"
    print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

# æ¯”è¼ƒè¡¨ç¤º
print("\n" + "=" * 70)
print("ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã«ã‚ˆã‚‹çµæœã®é•ã„ã€‘")
print("=" * 70)
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼ˆæ›–æ˜§ï¼‰:          {result_vague[:30]}")
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼ˆæ˜ç¢º+å½¢å¼ï¼‰:     {result_clear[:30]}")
print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³3ï¼ˆå½¹å‰²+Few+å½¢å¼ï¼‰: {result_best[:30]}")
print("\nğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå…·ä½“çš„ã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ã»ã©ã€æœŸå¾…ã™ã‚‹å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™")

# ============================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
# ============================================

print("\n\n" + "=" * 70)
print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹")
print("=" * 70)

best_practices = """
ã€1. æ˜ç¢ºã§å…·ä½“çš„ãªæŒ‡ç¤ºã€‘
âœ“ ã‚¿ã‚¹ã‚¯ã‚’æ˜ç¢ºã«å®šç¾©ã™ã‚‹
âœ“ æ›–æ˜§ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹
âœ— ã€Œæ„Ÿæƒ…ã‚’æ•™ãˆã¦ã€â†’ âœ“ ã€Œæ„Ÿæƒ…ã‚’ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ä¸­ç«‹ã§åˆ†é¡ã€

ã€2. å‡ºåŠ›å½¢å¼ã®æŒ‡å®šã€‘
âœ“ æœŸå¾…ã™ã‚‹å‡ºåŠ›ã®å½¢å¼ã‚’æ˜ç¤ºã™ã‚‹
âœ“ ç®‡æ¡æ›¸ãã€JSONã€å˜èªã®ã¿ãªã©
âœ— å½¢å¼æŒ‡å®šãªã— â†’ âœ“ ã€Œæ„Ÿæƒ…ã®ã¿ã‚’1å˜èªã§ç­”ãˆã¦ãã ã•ã„ã€

ã€3. å½¹å‰²ã®è¨­å®šã€‘
âœ“ ã€Œã‚ãªãŸã¯ã€œã®å°‚é–€å®¶ã§ã™ã€ã¨å½¹å‰²ã‚’ä¸ãˆã‚‹
âœ“ ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸæ–‡è„ˆã‚’æä¾›
ä¾‹: ã€Œã‚ãªãŸã¯æ„Ÿæƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€

ã€4. ä¾‹é¡Œã®æä¾›ï¼ˆFew-shotï¼‰ã€‘
âœ“ å…¥åŠ›ã¨æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ã®ä¾‹ã‚’ç¤ºã™
âœ“ å¤šæ§˜ãªä¾‹ã‚’2-5å€‹ç¨‹åº¦
âœ“ ä¾‹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®

ã€5. åˆ¶ç´„æ¡ä»¶ã®æ˜ç¤ºã€‘
âœ“ æ–‡å­—æ•°ã€ä½¿ç”¨èªå½™ã€ãƒˆãƒ¼ãƒ³ãªã©ã‚’æŒ‡å®š
ä¾‹: ã€Œ100æ–‡å­—ä»¥å†…ã§ã€ã€Œå°‚é–€ç”¨èªã‚’é¿ã‘ã¦ã€

ã€6. æ®µéšçš„æ€è€ƒã®ä¿ƒé€²ï¼ˆCoTï¼‰ã€‘
âœ“ ã€Œæ®µéšçš„ã«è€ƒãˆã¾ã—ã‚‡ã†ã€ã‚’è¿½åŠ 
âœ“ è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§åŠ¹æœçš„
âœ“ ä¾‹é¡Œã«æ€è€ƒéç¨‹ã‚’å«ã‚ã‚‹

ã€7. å¦å®šå½¢ã‚ˆã‚Šè‚¯å®šå½¢ã€‘
âœ— ã€Œã€œã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€
âœ“ ã€Œã€œã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„ã€
"""

print(best_practices)


# ============================================
# å‚è€ƒãƒªã‚½ãƒ¼ã‚¹
# ============================================

print("\n\n" + "=" * 70)
print("å‚è€ƒãƒªã‚½ãƒ¼ã‚¹ - ã•ã‚‰ã«å­¦ã¶ãŸã‚ã«")
print("=" * 70)

resources = """
ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‘
â€¢ OpenAI Prompt Engineering Guide
  https://platform.openai.com/docs/guides/prompt-engineering
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å…¬å¼ã‚¬ã‚¤ãƒ‰

ã€æŠ€è¡“è§£èª¬ã€‘
â€¢ Prompt Engineering by Lilian Weng
  https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/
  - åŒ…æ‹¬çš„ãªæŠ€è¡“è§£èª¬ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã€‘
â€¢ Prompt Engineering Guide (GitHub)
  https://github.com/dair-ai/Prompt-Engineering-Guide
  - è«–æ–‡ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€å®Ÿä¾‹ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
  - å¤šè¨€èªå¯¾å¿œï¼ˆæ—¥æœ¬èªã‚ã‚Šï¼‰

â€¢ Awesome ChatGPT Prompts
  https://github.com/f/awesome-chatgpt-prompts
  - æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹é›†
  - ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§å…±æœ‰ã•ã‚ŒãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
â€¢ Hugging Face Datasets
  https://huggingface.co/datasets
  - å…¬é–‹ã•ã‚Œã¦ã„ã‚‹å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
  - æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚è±Šå¯Œ

ã€ãƒ¢ãƒ‡ãƒ«ã€‘
â€¢ Hugging Face Model Hub
  https://huggingface.co/models
  - APIã‚­ãƒ¼ä¸è¦ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ãŒå¤šæ•°
  - æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚‚å……å®Ÿ
"""

print(resources)

# ============================================
# ã¾ã¨ã‚
# ============================================

print("\n\n" + "=" * 70)
print("æ¼”ç¿’ã®ã¾ã¨ã‚")
print("=" * 70)

summary = f"""
ã€æœ¬æ—¥å­¦ã‚“ã ã“ã¨ã€‘

1ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤
   âœ“ Zero-shot vs Few-shotã®é•ã„ã¨åŠ¹æœ
   âœ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆãŒå‡ºåŠ›å“è³ªã«å¤§ããå½±éŸ¿
   âœ“ æ˜ç¢ºãªæŒ‡ç¤ºã¨å½¢å¼æŒ‡å®šã®é‡è¦æ€§

2ï¸âƒ£ In-Context Learning
   âœ“ ä¾‹é¡Œã‹ã‚‰å­¦ç¿’ã™ã‚‹èƒ½åŠ›
   âœ“ ä¾‹é¡Œã®æ•°ã¨è³ªãŒæ€§èƒ½ã«å½±éŸ¿
   âœ“ é©åˆ‡ãªä¾‹é¡Œé¸æŠã®é‡è¦æ€§

3ï¸âƒ£ Chain-of-Thought Prompting
   âœ“ æ€è€ƒéç¨‹ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ã§æ¨è«–èƒ½åŠ›å‘ä¸Š
   âœ“ è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã§ç‰¹ã«åŠ¹æœçš„
   âœ“ Zero-shot CoTã§ã‚‚åŠ¹æœã‚ã‚Š

4ï¸âƒ£ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
   âœ“ å½¹å‰²è¨­å®šã€å½¢å¼æŒ‡å®šã€åˆ¶ç´„æ¡ä»¶ã®æ˜ç¤º
   âœ“ æ®µéšçš„æ€è€ƒã®ä¿ƒé€²
   âœ“ é©åˆ‡ãªä¾‹é¡Œã®æä¾›

ã€ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‘
ãƒ¢ãƒ‡ãƒ«: {model_name}
ç‰¹å¾´: {loaded_model_info["description"]}

ã€ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‘
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_info["name"]}
èª¬æ˜: {dataset_info["description"]}

ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
â€¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ŒæŒ‡ç¤ºæ›¸ã€- å…·ä½“çš„ã§æ˜ç¢ºã«
â€¢ ä¾‹é¡Œã¯ã€ŒãŠæ‰‹æœ¬ã€- å¤šæ§˜æ€§ã¨ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®
â€¢ CoTã¯ã€Œæ€è€ƒã®è¦‹ãˆã‚‹åŒ–ã€- æ¨è«–ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›
â€¢ å®Ÿé¨“ã¨æ”¹å–„ã®ã‚µã‚¤ã‚¯ãƒ«ãŒé‡è¦

ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘
â€¢ æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­è¨ˆã—ã¦ã¿ã‚‹
â€¢ å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡å®Ÿé¨“ã‚’è¡Œã†
â€¢ æœ€æ–°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã‚’å­¦ã¶ï¼ˆå‚è€ƒãƒªã‚½ãƒ¼ã‚¹å‚ç…§ï¼‰
â€¢ ç‹¬è‡ªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹
"""

print(summary)
